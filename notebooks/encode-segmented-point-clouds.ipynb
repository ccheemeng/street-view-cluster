{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Dockerfiles have not been modified, connect to the Jupyter server with:  \n",
    "- ```http://localhost:8013/tree?token=encode-segmented-point-clouds-cpu``` to run Torch on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"data\"\n",
    "input_dir = \"testcollection\"\n",
    "# input_dir = \"place-pulse-singapore-segmented-point-clouds-s3dis\"\n",
    "output_dir = \"place-pulse-singapore-segmented-point-clouds-encoded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sh compile_wrappers.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.architectures import KPFCNN\n",
    "from utils.config import Config\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPConv-PyTorch/datasets/S3DIS.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#      0=================================0\n",
    "#      |    Kernel Point Convolutions    |\n",
    "#      0=================================0\n",
    "#\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#      Class handling S3DIS dataset.\n",
    "#      Implements a Dataset, a Sampler, and a collate_fn\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#      Hugues THOMAS - 11/06/2018\n",
    "#\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Imports and global variables\n",
    "#       \\**********************************/\n",
    "#\n",
    "\n",
    "# Common libs\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import math\n",
    "import warnings\n",
    "from multiprocessing import Lock\n",
    "\n",
    "\n",
    "# OS functions\n",
    "from os import listdir\n",
    "from os.path import exists, join, isdir\n",
    "\n",
    "# Dataset parent class\n",
    "from datasets.common import PointCloudDataset\n",
    "from torch.utils.data import Sampler, get_worker_info\n",
    "from utils.mayavi_visu import *\n",
    "\n",
    "from datasets.common import grid_subsampling\n",
    "from utils.config import bcolors\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Dataset class definition\n",
    "#       \\******************************/\n",
    "\n",
    "\n",
    "class S3DISDataset(PointCloudDataset):\n",
    "    \"\"\"Class to handle S3DIS dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, config, path=\"../../Data/S3DIS\", cloud_names=['Area_1', 'Area_2', 'Area_3', 'Area_4', 'Area_5', 'Area_6'], set='training', use_potentials=True, load_data=True):\n",
    "        \"\"\"\n",
    "        This dataset is small enough to be stored in-memory, so load all point clouds here\n",
    "        \"\"\"\n",
    "        PointCloudDataset.__init__(self, 'S3DIS')\n",
    "\n",
    "        ############\n",
    "        # Parameters\n",
    "        ############\n",
    "\n",
    "        # Dict from labels to names\n",
    "        self.label_to_names = {0: 'ceiling',\n",
    "                               1: 'floor',\n",
    "                               2: 'wall',\n",
    "                               3: 'beam',\n",
    "                               4: 'column',\n",
    "                               5: 'window',\n",
    "                               6: 'door',\n",
    "                               7: 'chair',\n",
    "                               8: 'table',\n",
    "                               9: 'bookcase',\n",
    "                               10: 'sofa',\n",
    "                               11: 'board',\n",
    "                               12: 'clutter'}\n",
    "\n",
    "        # Initialize a bunch of variables concerning class labels\n",
    "        self.init_labels()\n",
    "\n",
    "        # List of classes ignored during training (can be empty)\n",
    "        self.ignored_labels = np.array([])\n",
    "\n",
    "        # Dataset folder\n",
    "        self.path = path\n",
    "\n",
    "        # Type of task conducted on this dataset\n",
    "        self.dataset_task = 'cloud_segmentation'\n",
    "\n",
    "        # Update number of class and data task in configuration\n",
    "        config.num_classes = self.num_classes - len(self.ignored_labels)\n",
    "        config.dataset_task = self.dataset_task\n",
    "\n",
    "        # Parameters from config\n",
    "        self.config = config\n",
    "\n",
    "        # Training or test set\n",
    "        self.set = set\n",
    "\n",
    "        # Using potential or random epoch generation\n",
    "        self.use_potentials = use_potentials\n",
    "\n",
    "        # Path of the training files\n",
    "        self.train_path = 'original_ply'\n",
    "\n",
    "        # List of files to process\n",
    "        ply_path = join(self.path, self.train_path)\n",
    "\n",
    "        # Proportion of validation scenes\n",
    "        self.cloud_names = cloud_names\n",
    "        self.all_splits = [0]\n",
    "        self.validation_split = 0\n",
    "\n",
    "        # Number of models used per epoch\n",
    "        if self.set == 'training':\n",
    "            self.epoch_n = config.epoch_steps * config.batch_num\n",
    "        elif self.set in ['validation', 'test', 'ERF']:\n",
    "            self.epoch_n = config.validation_size * config.batch_num\n",
    "        else:\n",
    "            raise ValueError('Unknown set for S3DIS data: ', self.set)\n",
    "\n",
    "        # Stop data is not needed\n",
    "        if not load_data:\n",
    "            return\n",
    "\n",
    "        ###################\n",
    "        # Prepare ply files\n",
    "        ###################\n",
    "\n",
    "        self.prepare_S3DIS_ply()\n",
    "\n",
    "        ################\n",
    "        # Load ply files\n",
    "        ################\n",
    "\n",
    "        # List of training files\n",
    "        self.files = []\n",
    "        for i, f in enumerate(self.cloud_names):\n",
    "            if self.set == 'training':\n",
    "                if self.all_splits[i] != self.validation_split:\n",
    "                    self.files += [join(ply_path, f + '.ply')]\n",
    "            elif self.set in ['validation', 'test', 'ERF']:\n",
    "                if self.all_splits[i] == self.validation_split:\n",
    "                    self.files += [join(ply_path, f + '.ply')]\n",
    "            else:\n",
    "                raise ValueError('Unknown set for S3DIS data: ', self.set)\n",
    "\n",
    "        if self.set == 'training':\n",
    "            self.cloud_names = [f for i, f in enumerate(self.cloud_names)\n",
    "                                if self.all_splits[i] != self.validation_split]\n",
    "        elif self.set in ['validation', 'test', 'ERF']:\n",
    "            self.cloud_names = [f for i, f in enumerate(self.cloud_names)\n",
    "                                if self.all_splits[i] == self.validation_split]\n",
    "\n",
    "        if 0 < self.config.first_subsampling_dl <= 0.01:\n",
    "            raise ValueError('subsampling_parameter too low (should be over 1 cm')\n",
    "\n",
    "        # Initiate containers\n",
    "        self.input_trees = []\n",
    "        self.input_colors = []\n",
    "        self.input_labels = []\n",
    "        self.pot_trees = []\n",
    "        self.num_clouds = 0\n",
    "        self.test_proj = []\n",
    "        self.validation_labels = []\n",
    "\n",
    "        # Start loading\n",
    "        self.load_subsampled_clouds()\n",
    "\n",
    "        ############################\n",
    "        # Batch selection parameters\n",
    "        ############################\n",
    "\n",
    "        # Initialize value for batch limit (max number of points per batch).\n",
    "        self.batch_limit = torch.tensor([1], dtype=torch.float32)\n",
    "        self.batch_limit.share_memory_()\n",
    "\n",
    "        # Initialize potentials\n",
    "        if use_potentials:\n",
    "            self.potentials = []\n",
    "            self.min_potentials = []\n",
    "            self.argmin_potentials = []\n",
    "            for i, tree in enumerate(self.pot_trees):\n",
    "                self.potentials += [torch.from_numpy(np.random.rand(tree.data.shape[0]) * 1e-3)]\n",
    "                min_ind = int(torch.argmin(self.potentials[-1]))\n",
    "                self.argmin_potentials += [min_ind]\n",
    "                self.min_potentials += [float(self.potentials[-1][min_ind])]\n",
    "\n",
    "            # Share potential memory\n",
    "            self.argmin_potentials = torch.from_numpy(np.array(self.argmin_potentials, dtype=np.int64))\n",
    "            self.min_potentials = torch.from_numpy(np.array(self.min_potentials, dtype=np.float64))\n",
    "            self.argmin_potentials.share_memory_()\n",
    "            self.min_potentials.share_memory_()\n",
    "            for i, _ in enumerate(self.pot_trees):\n",
    "                self.potentials[i].share_memory_()\n",
    "\n",
    "            self.worker_waiting = torch.tensor([0 for _ in range(config.input_threads)], dtype=torch.int32)\n",
    "            self.worker_waiting.share_memory_()\n",
    "            self.epoch_inds = None\n",
    "            self.epoch_i = 0\n",
    "\n",
    "        else:\n",
    "            self.potentials = None\n",
    "            self.min_potentials = None\n",
    "            self.argmin_potentials = None\n",
    "            self.epoch_inds = torch.from_numpy(np.zeros((2, self.epoch_n), dtype=np.int64))\n",
    "            self.epoch_i = torch.from_numpy(np.zeros((1,), dtype=np.int64))\n",
    "            self.epoch_i.share_memory_()\n",
    "            self.epoch_inds.share_memory_()\n",
    "\n",
    "        self.worker_lock = Lock()\n",
    "\n",
    "        # For ERF visualization, we want only one cloud per batch and no randomness\n",
    "        if self.set == 'ERF':\n",
    "            self.batch_limit = torch.tensor([1], dtype=torch.float32)\n",
    "            self.batch_limit.share_memory_()\n",
    "            np.random.seed(42)\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of data here\n",
    "        \"\"\"\n",
    "        return len(self.cloud_names)\n",
    "\n",
    "    def __getitem__(self, batch_i):\n",
    "        \"\"\"\n",
    "        The main thread gives a list of indices to load a batch. Each worker is going to work in parallel to load a\n",
    "        different list of indices.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.use_potentials:\n",
    "            return self.potential_item(batch_i)\n",
    "        else:\n",
    "            return self.random_item(batch_i)\n",
    "\n",
    "    def potential_item(self, batch_i, debug_workers=False):\n",
    "\n",
    "        t = [time.time()]\n",
    "\n",
    "        # Initiate concatanation lists\n",
    "        p_list = []\n",
    "        f_list = []\n",
    "        l_list = []\n",
    "        i_list = []\n",
    "        pi_list = []\n",
    "        ci_list = []\n",
    "        s_list = []\n",
    "        R_list = []\n",
    "        batch_n = 0\n",
    "        failed_attempts = 0\n",
    "\n",
    "        info = get_worker_info()\n",
    "        if info is not None:\n",
    "            wid = info.id\n",
    "        else:\n",
    "            wid = None\n",
    "\n",
    "        while True:\n",
    "\n",
    "            t += [time.time()]\n",
    "\n",
    "            if debug_workers:\n",
    "                message = ''\n",
    "                for wi in range(info.num_workers):\n",
    "                    if wi == wid:\n",
    "                        message += ' {:}X{:} '.format(bcolors.FAIL, bcolors.ENDC)\n",
    "                    elif self.worker_waiting[wi] == 0:\n",
    "                        message += '   '\n",
    "                    elif self.worker_waiting[wi] == 1:\n",
    "                        message += ' | '\n",
    "                    elif self.worker_waiting[wi] == 2:\n",
    "                        message += ' o '\n",
    "                print(message)\n",
    "                self.worker_waiting[wid] = 0\n",
    "\n",
    "            with self.worker_lock:\n",
    "\n",
    "                if debug_workers:\n",
    "                    message = ''\n",
    "                    for wi in range(info.num_workers):\n",
    "                        if wi == wid:\n",
    "                            message += ' {:}v{:} '.format(bcolors.OKGREEN, bcolors.ENDC)\n",
    "                        elif self.worker_waiting[wi] == 0:\n",
    "                            message += '   '\n",
    "                        elif self.worker_waiting[wi] == 1:\n",
    "                            message += ' | '\n",
    "                        elif self.worker_waiting[wi] == 2:\n",
    "                            message += ' o '\n",
    "                    print(message)\n",
    "                    self.worker_waiting[wid] = 1\n",
    "\n",
    "                # Get potential minimum\n",
    "                cloud_ind = int(torch.argmin(self.min_potentials))\n",
    "                point_ind = int(self.argmin_potentials[cloud_ind])\n",
    "\n",
    "                # Get potential points from tree structure\n",
    "                pot_points = np.array(self.pot_trees[cloud_ind].data, copy=False)\n",
    "\n",
    "                # Center point of input region\n",
    "                center_point = np.copy(pot_points[point_ind, :].reshape(1, -1))\n",
    "\n",
    "                # Add a small noise to center point\n",
    "                if self.set != 'ERF':\n",
    "                    center_point += np.clip(np.random.normal(scale=self.config.in_radius / 10, size=center_point.shape),\n",
    "                                            -self.config.in_radius / 2,\n",
    "                                            self.config.in_radius / 2)\n",
    "\n",
    "                # Indices of points in input region\n",
    "                pot_inds, dists = self.pot_trees[cloud_ind].query_radius(center_point,\n",
    "                                                                         r=self.config.in_radius,\n",
    "                                                                         return_distance=True)\n",
    "\n",
    "                d2s = np.square(dists[0])\n",
    "                pot_inds = pot_inds[0]\n",
    "\n",
    "                # Update potentials (Tukey weights)\n",
    "                if self.set != 'ERF':\n",
    "                    tukeys = np.square(1 - d2s / np.square(self.config.in_radius))\n",
    "                    tukeys[d2s > np.square(self.config.in_radius)] = 0\n",
    "                    self.potentials[cloud_ind][pot_inds] += tukeys\n",
    "                    min_ind = torch.argmin(self.potentials[cloud_ind])\n",
    "                    self.min_potentials[[cloud_ind]] = self.potentials[cloud_ind][min_ind]\n",
    "                    self.argmin_potentials[[cloud_ind]] = min_ind\n",
    "\n",
    "            t += [time.time()]\n",
    "\n",
    "            # Get points from tree structure\n",
    "            points = np.array(self.input_trees[cloud_ind].data, copy=False)\n",
    "\n",
    "\n",
    "            # Indices of points in input region\n",
    "            input_inds = self.input_trees[cloud_ind].query_radius(center_point,\n",
    "                                                                  r=self.config.in_radius)[0]\n",
    "\n",
    "            t += [time.time()]\n",
    "\n",
    "            # Number collected\n",
    "            n = input_inds.shape[0]\n",
    "\n",
    "            # Safe check for empty spheres\n",
    "            if n < 2:\n",
    "                failed_attempts += 1\n",
    "                if failed_attempts > 100 * self.config.batch_num:\n",
    "                    raise ValueError('It seems this dataset only containes empty input spheres')\n",
    "                t += [time.time()]\n",
    "                t += [time.time()]\n",
    "                continue\n",
    "\n",
    "            # Collect labels and colors\n",
    "            input_points = (points[input_inds] - center_point).astype(np.float32)\n",
    "            input_colors = self.input_colors[cloud_ind][input_inds]\n",
    "            if self.set in ['test', 'ERF']:\n",
    "                input_labels = np.zeros(input_points.shape[0])\n",
    "            else:\n",
    "                input_labels = self.input_labels[cloud_ind][input_inds]\n",
    "                input_labels = np.array([self.label_to_idx[l] for l in input_labels])\n",
    "\n",
    "            t += [time.time()]\n",
    "\n",
    "            # Data augmentation\n",
    "            input_points, scale, R = self.augmentation_transform(input_points)\n",
    "\n",
    "            # Color augmentation\n",
    "            if np.random.rand() > self.config.augment_color:\n",
    "                input_colors *= 0\n",
    "\n",
    "            # Get original height as additional feature\n",
    "            input_features = np.hstack((input_colors, input_points[:, 2:] + center_point[:, 2:])).astype(np.float32)\n",
    "\n",
    "            t += [time.time()]\n",
    "\n",
    "            # Stack batch\n",
    "            p_list += [input_points]\n",
    "            f_list += [input_features]\n",
    "            l_list += [input_labels]\n",
    "            pi_list += [input_inds]\n",
    "            i_list += [point_ind]\n",
    "            ci_list += [cloud_ind]\n",
    "            s_list += [scale]\n",
    "            R_list += [R]\n",
    "\n",
    "            # Update batch size\n",
    "            batch_n += n\n",
    "\n",
    "            # In case batch is full, stop\n",
    "            if batch_n > int(self.batch_limit):\n",
    "                break\n",
    "\n",
    "            # Randomly drop some points (act as an augmentation process and a safety for GPU memory consumption)\n",
    "            # if n > int(self.batch_limit):\n",
    "            #    input_inds = np.random.choice(input_inds, size=int(self.batch_limit) - 1, replace=False)\n",
    "            #    n = input_inds.shape[0]\n",
    "\n",
    "        ###################\n",
    "        # Concatenate batch\n",
    "        ###################\n",
    "\n",
    "        stacked_points = np.concatenate(p_list, axis=0)\n",
    "        features = np.concatenate(f_list, axis=0)\n",
    "        labels = np.concatenate(l_list, axis=0)\n",
    "        point_inds = np.array(i_list, dtype=np.int32)\n",
    "        cloud_inds = np.array(ci_list, dtype=np.int32)\n",
    "        input_inds = np.concatenate(pi_list, axis=0)\n",
    "        stack_lengths = np.array([pp.shape[0] for pp in p_list], dtype=np.int32)\n",
    "        scales = np.array(s_list, dtype=np.float32)\n",
    "        rots = np.stack(R_list, axis=0)\n",
    "\n",
    "        # Input features\n",
    "        stacked_features = np.ones_like(stacked_points[:, :1], dtype=np.float32)\n",
    "        if self.config.in_features_dim == 1:\n",
    "            pass\n",
    "        elif self.config.in_features_dim == 4:\n",
    "            stacked_features = np.hstack((stacked_features, features[:, :3]))\n",
    "        elif self.config.in_features_dim == 5:\n",
    "            stacked_features = np.hstack((stacked_features, features))\n",
    "        else:\n",
    "            raise ValueError('Only accepted input dimensions are 1, 4 and 7 (without and with XYZ)')\n",
    "\n",
    "        #######################\n",
    "        # Create network inputs\n",
    "        #######################\n",
    "        #\n",
    "        #   Points, neighbors, pooling indices for each layers\n",
    "        #\n",
    "\n",
    "        t += [time.time()]\n",
    "\n",
    "        # Get the whole input list\n",
    "        input_list = self.segmentation_inputs(stacked_points,\n",
    "                                              stacked_features,\n",
    "                                              labels,\n",
    "                                              stack_lengths)\n",
    "\n",
    "        t += [time.time()]\n",
    "\n",
    "        # Add scale and rotation for testing\n",
    "        input_list += [scales, rots, cloud_inds, point_inds, input_inds]\n",
    "\n",
    "        if debug_workers:\n",
    "            message = ''\n",
    "            for wi in range(info.num_workers):\n",
    "                if wi == wid:\n",
    "                    message += ' {:}0{:} '.format(bcolors.OKBLUE, bcolors.ENDC)\n",
    "                elif self.worker_waiting[wi] == 0:\n",
    "                    message += '   '\n",
    "                elif self.worker_waiting[wi] == 1:\n",
    "                    message += ' | '\n",
    "                elif self.worker_waiting[wi] == 2:\n",
    "                    message += ' o '\n",
    "            print(message)\n",
    "            self.worker_waiting[wid] = 2\n",
    "\n",
    "        t += [time.time()]\n",
    "\n",
    "        # Display timings\n",
    "        debugT = False\n",
    "        if debugT:\n",
    "            print('\\n************************\\n')\n",
    "            print('Timings:')\n",
    "            ti = 0\n",
    "            N = 5\n",
    "            mess = 'Init ...... {:5.1f}ms /'\n",
    "            loop_times = [1000 * (t[ti + N * i + 1] - t[ti + N * i]) for i in range(len(stack_lengths))]\n",
    "            for dt in loop_times:\n",
    "                mess += ' {:5.1f}'.format(dt)\n",
    "            print(mess.format(np.sum(loop_times)))\n",
    "            ti += 1\n",
    "            mess = 'Pots ...... {:5.1f}ms /'\n",
    "            loop_times = [1000 * (t[ti + N * i + 1] - t[ti + N * i]) for i in range(len(stack_lengths))]\n",
    "            for dt in loop_times:\n",
    "                mess += ' {:5.1f}'.format(dt)\n",
    "            print(mess.format(np.sum(loop_times)))\n",
    "            ti += 1\n",
    "            mess = 'Sphere .... {:5.1f}ms /'\n",
    "            loop_times = [1000 * (t[ti + N * i + 1] - t[ti + N * i]) for i in range(len(stack_lengths))]\n",
    "            for dt in loop_times:\n",
    "                mess += ' {:5.1f}'.format(dt)\n",
    "            print(mess.format(np.sum(loop_times)))\n",
    "            ti += 1\n",
    "            mess = 'Collect ... {:5.1f}ms /'\n",
    "            loop_times = [1000 * (t[ti + N * i + 1] - t[ti + N * i]) for i in range(len(stack_lengths))]\n",
    "            for dt in loop_times:\n",
    "                mess += ' {:5.1f}'.format(dt)\n",
    "            print(mess.format(np.sum(loop_times)))\n",
    "            ti += 1\n",
    "            mess = 'Augment ... {:5.1f}ms /'\n",
    "            loop_times = [1000 * (t[ti + N * i + 1] - t[ti + N * i]) for i in range(len(stack_lengths))]\n",
    "            for dt in loop_times:\n",
    "                mess += ' {:5.1f}'.format(dt)\n",
    "            print(mess.format(np.sum(loop_times)))\n",
    "            ti += N * (len(stack_lengths) - 1) + 1\n",
    "            print('concat .... {:5.1f}ms'.format(1000 * (t[ti+1] - t[ti])))\n",
    "            ti += 1\n",
    "            print('input ..... {:5.1f}ms'.format(1000 * (t[ti+1] - t[ti])))\n",
    "            ti += 1\n",
    "            print('stack ..... {:5.1f}ms'.format(1000 * (t[ti+1] - t[ti])))\n",
    "            ti += 1\n",
    "            print('\\n************************\\n')\n",
    "        return input_list\n",
    "\n",
    "    def random_item(self, batch_i):\n",
    "\n",
    "        # Initiate concatanation lists\n",
    "        p_list = []\n",
    "        f_list = []\n",
    "        l_list = []\n",
    "        i_list = []\n",
    "        pi_list = []\n",
    "        ci_list = []\n",
    "        s_list = []\n",
    "        R_list = []\n",
    "        batch_n = 0\n",
    "        failed_attempts = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            with self.worker_lock:\n",
    "\n",
    "                # Get potential minimum\n",
    "                cloud_ind = int(self.epoch_inds[0, self.epoch_i])\n",
    "                point_ind = int(self.epoch_inds[1, self.epoch_i])\n",
    "\n",
    "                # Update epoch indice\n",
    "                self.epoch_i += 1\n",
    "                if self.epoch_i >= int(self.epoch_inds.shape[1]):\n",
    "                    self.epoch_i -= int(self.epoch_inds.shape[1])\n",
    "                \n",
    "\n",
    "            # Get points from tree structure\n",
    "            points = np.array(self.input_trees[cloud_ind].data, copy=False)\n",
    "\n",
    "            # Center point of input region\n",
    "            center_point = np.copy(points[point_ind, :].reshape(1, -1))\n",
    "\n",
    "            # Add a small noise to center point\n",
    "            if self.set != 'ERF':\n",
    "                center_point += np.clip(np.random.normal(scale=self.config.in_radius / 10, size=center_point.shape),\n",
    "                                        -self.config.in_radius / 2,\n",
    "                                        self.config.in_radius / 2)\n",
    "\n",
    "            # Indices of points in input region\n",
    "            input_inds = self.input_trees[cloud_ind].query_radius(center_point,\n",
    "                                                                  r=self.config.in_radius)[0]\n",
    "\n",
    "            # Number collected\n",
    "            n = input_inds.shape[0]\n",
    "            \n",
    "            # Safe check for empty spheres\n",
    "            if n < 2:\n",
    "                failed_attempts += 1\n",
    "                if failed_attempts > 100 * self.config.batch_num:\n",
    "                    raise ValueError('It seems this dataset only containes empty input spheres')\n",
    "                continue\n",
    "\n",
    "            # Collect labels and colors\n",
    "            input_points = (points[input_inds] - center_point).astype(np.float32)\n",
    "            input_colors = self.input_colors[cloud_ind][input_inds]\n",
    "            if self.set in ['test', 'ERF']:\n",
    "                input_labels = np.zeros(input_points.shape[0])\n",
    "            else:\n",
    "                input_labels = self.input_labels[cloud_ind][input_inds]\n",
    "                input_labels = np.array([self.label_to_idx[l] for l in input_labels])\n",
    "\n",
    "            # Data augmentation\n",
    "            input_points, scale, R = self.augmentation_transform(input_points)\n",
    "\n",
    "            # Color augmentation\n",
    "            if np.random.rand() > self.config.augment_color:\n",
    "                input_colors *= 0\n",
    "\n",
    "            # Get original height as additional feature\n",
    "            input_features = np.hstack((input_colors, input_points[:, 2:] + center_point[:, 2:])).astype(np.float32)\n",
    "\n",
    "            # Stack batch\n",
    "            p_list += [input_points]\n",
    "            f_list += [input_features]\n",
    "            l_list += [input_labels]\n",
    "            pi_list += [input_inds]\n",
    "            i_list += [point_ind]\n",
    "            ci_list += [cloud_ind]\n",
    "            s_list += [scale]\n",
    "            R_list += [R]\n",
    "\n",
    "            # Update batch size\n",
    "            batch_n += n\n",
    "\n",
    "            # In case batch is full, stop\n",
    "            if batch_n > int(self.batch_limit):\n",
    "                break\n",
    "\n",
    "            # Randomly drop some points (act as an augmentation process and a safety for GPU memory consumption)\n",
    "            # if n > int(self.batch_limit):\n",
    "            #    input_inds = np.random.choice(input_inds, size=int(self.batch_limit) - 1, replace=False)\n",
    "            #    n = input_inds.shape[0]\n",
    "\n",
    "        ###################\n",
    "        # Concatenate batch\n",
    "        ###################\n",
    "\n",
    "        stacked_points = np.concatenate(p_list, axis=0)\n",
    "        features = np.concatenate(f_list, axis=0)\n",
    "        labels = np.concatenate(l_list, axis=0)\n",
    "        point_inds = np.array(i_list, dtype=np.int32)\n",
    "        cloud_inds = np.array(ci_list, dtype=np.int32)\n",
    "        input_inds = np.concatenate(pi_list, axis=0)\n",
    "        stack_lengths = np.array([pp.shape[0] for pp in p_list], dtype=np.int32)\n",
    "        scales = np.array(s_list, dtype=np.float32)\n",
    "        rots = np.stack(R_list, axis=0)\n",
    "\n",
    "        # Input features\n",
    "        stacked_features = np.ones_like(stacked_points[:, :1], dtype=np.float32)\n",
    "        if self.config.in_features_dim == 1:\n",
    "            pass\n",
    "        elif self.config.in_features_dim == 4:\n",
    "            stacked_features = np.hstack((stacked_features, features[:, :3]))\n",
    "        elif self.config.in_features_dim == 5:\n",
    "            stacked_features = np.hstack((stacked_features, features))\n",
    "        else:\n",
    "            raise ValueError('Only accepted input dimensions are 1, 4 and 7 (without and with XYZ)')\n",
    "\n",
    "        #######################\n",
    "        # Create network inputs\n",
    "        #######################\n",
    "        #\n",
    "        #   Points, neighbors, pooling indices for each layers\n",
    "        #\n",
    "\n",
    "        # Get the whole input list\n",
    "        input_list = self.segmentation_inputs(stacked_points,\n",
    "                                              stacked_features,\n",
    "                                              labels,\n",
    "                                              stack_lengths)\n",
    "\n",
    "        # Add scale and rotation for testing\n",
    "        input_list += [scales, rots, cloud_inds, point_inds, input_inds]\n",
    "\n",
    "        return input_list\n",
    "\n",
    "    def prepare_S3DIS_ply(self):\n",
    "\n",
    "        print('\\nPreparing ply files')\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Folder for the ply files\n",
    "        ply_path = join(self.path, self.train_path)\n",
    "        if not exists(ply_path):\n",
    "            makedirs(ply_path)\n",
    "\n",
    "        for cloud_name in self.cloud_names:\n",
    "\n",
    "            # Pass if the cloud has already been computed\n",
    "            cloud_file = join(ply_path, cloud_name + '.ply')\n",
    "            if exists(cloud_file):\n",
    "                continue\n",
    "\n",
    "            # Get rooms of the current cloud\n",
    "            cloud_folder = join(self.path, cloud_name)\n",
    "            room_folders = [join(cloud_folder, room) for room in listdir(cloud_folder) if isdir(join(cloud_folder, room))]\n",
    "\n",
    "            # Initiate containers\n",
    "            cloud_points = np.empty((0, 3), dtype=np.float32)\n",
    "            cloud_colors = np.empty((0, 3), dtype=np.uint8)\n",
    "            cloud_classes = np.empty((0, 1), dtype=np.int32)\n",
    "\n",
    "            # Loop over rooms\n",
    "            for i, room_folder in enumerate(room_folders):\n",
    "\n",
    "                print('Cloud %s - Room %d/%d : %s' % (cloud_name, i+1, len(room_folders), room_folder.split('/')[-1]))\n",
    "\n",
    "                for object_name in listdir(join(room_folder, 'Annotations')):\n",
    "\n",
    "                    if object_name[-4:] == '.txt':\n",
    "\n",
    "                        # Text file containing point of the object\n",
    "                        object_file = join(room_folder, 'Annotations', object_name)\n",
    "\n",
    "                        # Object class and ID\n",
    "                        tmp = object_name[:-4].split('_')[0]\n",
    "                        if tmp in self.name_to_label:\n",
    "                            object_class = self.name_to_label[tmp]\n",
    "                        elif tmp in ['stairs']:\n",
    "                            object_class = self.name_to_label['clutter']\n",
    "                        else:\n",
    "                            raise ValueError('Unknown object name: ' + str(tmp))\n",
    "\n",
    "                        # Correct bug in S3DIS dataset\n",
    "                        if object_name == 'ceiling_1.txt':\n",
    "                            with open(object_file, 'r') as f:\n",
    "                                lines = f.readlines()\n",
    "                            for l_i, line in enumerate(lines):\n",
    "                                if '103.0\\x100000' in line:\n",
    "                                    lines[l_i] = line.replace('103.0\\x100000', '103.000000')\n",
    "                            with open(object_file, 'w') as f:\n",
    "                                f.writelines(lines)\n",
    "\n",
    "                        # Read object points and colors\n",
    "                        object_data = np.loadtxt(object_file, dtype=np.float32)\n",
    "\n",
    "                        # Stack all data\n",
    "                        cloud_points = np.vstack((cloud_points, object_data[:, 0:3].astype(np.float32)))\n",
    "                        cloud_colors = np.vstack((cloud_colors, object_data[:, 3:6].astype(np.uint8)))\n",
    "                        object_classes = np.full((object_data.shape[0], 1), object_class, dtype=np.int32)\n",
    "                        cloud_classes = np.vstack((cloud_classes, object_classes))\n",
    "\n",
    "            # Save as ply\n",
    "            write_ply(cloud_file,\n",
    "                      (cloud_points, cloud_colors, cloud_classes),\n",
    "                      ['x', 'y', 'z', 'red', 'green', 'blue', 'class'])\n",
    "\n",
    "        print('Done in {:.1f}s'.format(time.time() - t0))\n",
    "        return\n",
    "\n",
    "    def load_subsampled_clouds(self):\n",
    "\n",
    "        # Parameter\n",
    "        dl = self.config.first_subsampling_dl\n",
    "\n",
    "        # Create path for files\n",
    "        tree_path = join(self.path, 'input_{:.3f}'.format(dl))\n",
    "        if not exists(tree_path):\n",
    "            makedirs(tree_path)\n",
    "\n",
    "        ##############\n",
    "        # Load KDTrees\n",
    "        ##############\n",
    "\n",
    "        for i, file_path in enumerate(self.files):\n",
    "\n",
    "            # Restart timer\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Get cloud name\n",
    "            cloud_name = self.cloud_names[i]\n",
    "\n",
    "            # Name of the input files\n",
    "            KDTree_file = join(tree_path, '{:s}_KDTree.pkl'.format(cloud_name))\n",
    "            sub_ply_file = join(tree_path, '{:s}.ply'.format(cloud_name))\n",
    "\n",
    "            # Check if inputs have already been computed\n",
    "            if exists(KDTree_file):\n",
    "                print('\\nFound KDTree for cloud {:s}, subsampled at {:.3f}'.format(cloud_name, dl))\n",
    "\n",
    "                # read ply with data\n",
    "                data = read_ply(sub_ply_file)\n",
    "                sub_colors = np.vstack((data['red'], data['green'], data['blue'])).T\n",
    "                sub_labels = data['class']\n",
    "\n",
    "                # Read pkl with search tree\n",
    "                with open(KDTree_file, 'rb') as f:\n",
    "                    search_tree = pickle.load(f)\n",
    "\n",
    "            else:\n",
    "                print('\\nPreparing KDTree for cloud {:s}, subsampled at {:.3f}'.format(cloud_name, dl))\n",
    "\n",
    "                # Read ply file\n",
    "                data = read_ply(file_path)\n",
    "                points = np.vstack((data['x'], data['y'], data['z'])).T\n",
    "                colors = np.vstack((data['red'], data['green'], data['blue'])).T\n",
    "                labels = data['class']\n",
    "\n",
    "                # Subsample cloud\n",
    "                sub_points, sub_colors, sub_labels = grid_subsampling(points,\n",
    "                                                                      features=colors,\n",
    "                                                                      labels=labels,\n",
    "                                                                      sampleDl=dl)\n",
    "\n",
    "                # Rescale float color and squeeze label\n",
    "                sub_colors = sub_colors / 255\n",
    "                sub_labels = np.squeeze(sub_labels)\n",
    "\n",
    "                # Get chosen neighborhoods\n",
    "                search_tree = KDTree(sub_points, leaf_size=10)\n",
    "                #search_tree = nnfln.KDTree(n_neighbors=1, metric='L2', leaf_size=10)\n",
    "                #search_tree.fit(sub_points)\n",
    "\n",
    "                # Save KDTree\n",
    "                with open(KDTree_file, 'wb') as f:\n",
    "                    pickle.dump(search_tree, f)\n",
    "\n",
    "                # Save ply\n",
    "                write_ply(sub_ply_file,\n",
    "                          [sub_points, sub_colors, sub_labels],\n",
    "                          ['x', 'y', 'z', 'red', 'green', 'blue', 'class'])\n",
    "\n",
    "            # Fill data containers\n",
    "            self.input_trees += [search_tree]\n",
    "            self.input_colors += [sub_colors]\n",
    "            self.input_labels += [sub_labels]\n",
    "\n",
    "            size = sub_colors.shape[0] * 4 * 7\n",
    "            print('{:.1f} MB loaded in {:.1f}s'.format(size * 1e-6, time.time() - t0))\n",
    "\n",
    "        ############################\n",
    "        # Coarse potential locations\n",
    "        ############################\n",
    "\n",
    "        # Only necessary for validation and test sets\n",
    "        if self.use_potentials:\n",
    "            print('\\nPreparing potentials')\n",
    "\n",
    "            # Restart timer\n",
    "            t0 = time.time()\n",
    "\n",
    "            pot_dl = self.config.in_radius / 10\n",
    "            cloud_ind = 0\n",
    "\n",
    "            for i, file_path in enumerate(self.files):\n",
    "\n",
    "                # Get cloud name\n",
    "                cloud_name = self.cloud_names[i]\n",
    "\n",
    "                # Name of the input files\n",
    "                coarse_KDTree_file = join(tree_path, '{:s}_coarse_KDTree.pkl'.format(cloud_name))\n",
    "\n",
    "                # Check if inputs have already been computed\n",
    "                if exists(coarse_KDTree_file):\n",
    "                    # Read pkl with search tree\n",
    "                    with open(coarse_KDTree_file, 'rb') as f:\n",
    "                        search_tree = pickle.load(f)\n",
    "\n",
    "                else:\n",
    "                    # Subsample cloud\n",
    "                    sub_points = np.array(self.input_trees[cloud_ind].data, copy=False)\n",
    "                    coarse_points = grid_subsampling(sub_points.astype(np.float32), sampleDl=pot_dl)\n",
    "\n",
    "                    # Get chosen neighborhoods\n",
    "                    search_tree = KDTree(coarse_points, leaf_size=10)\n",
    "\n",
    "                    # Save KDTree\n",
    "                    with open(coarse_KDTree_file, 'wb') as f:\n",
    "                        pickle.dump(search_tree, f)\n",
    "\n",
    "                # Fill data containers\n",
    "                self.pot_trees += [search_tree]\n",
    "                cloud_ind += 1\n",
    "\n",
    "            print('Done in {:.1f}s'.format(time.time() - t0))\n",
    "\n",
    "        ######################\n",
    "        # Reprojection indices\n",
    "        ######################\n",
    "\n",
    "        # Get number of clouds\n",
    "        self.num_clouds = len(self.input_trees)\n",
    "\n",
    "        # Only necessary for validation and test sets\n",
    "        if self.set in ['validation', 'test']:\n",
    "\n",
    "            print('\\nPreparing reprojection indices for testing')\n",
    "\n",
    "            # Get validation/test reprojection indices\n",
    "            for i, file_path in enumerate(self.files):\n",
    "\n",
    "                # Restart timer\n",
    "                t0 = time.time()\n",
    "\n",
    "                # Get info on this cloud\n",
    "                cloud_name = self.cloud_names[i]\n",
    "\n",
    "                # File name for saving\n",
    "                proj_file = join(tree_path, '{:s}_proj.pkl'.format(cloud_name))\n",
    "\n",
    "                # Try to load previous indices\n",
    "                if exists(proj_file):\n",
    "                    with open(proj_file, 'rb') as f:\n",
    "                        proj_inds, labels = pickle.load(f)\n",
    "                else:\n",
    "                    data = read_ply(file_path)\n",
    "                    points = np.vstack((data['x'], data['y'], data['z'])).T\n",
    "                    labels = data['class']\n",
    "\n",
    "                    # Compute projection inds\n",
    "                    idxs = self.input_trees[i].query(points, return_distance=False)\n",
    "                    #dists, idxs = self.input_trees[i_cloud].kneighbors(points)\n",
    "                    proj_inds = np.squeeze(idxs).astype(np.int32)\n",
    "\n",
    "                    # Save\n",
    "                    with open(proj_file, 'wb') as f:\n",
    "                        pickle.dump([proj_inds, labels], f)\n",
    "\n",
    "                self.test_proj += [proj_inds]\n",
    "                self.validation_labels += [labels]\n",
    "                print('{:s} done in {:.1f}s'.format(cloud_name, time.time() - t0))\n",
    "\n",
    "        print()\n",
    "        return\n",
    "\n",
    "    def load_evaluation_points(self, file_path):\n",
    "        \"\"\"\n",
    "        Load points (from test or validation split) on which the metrics should be evaluated\n",
    "        \"\"\"\n",
    "\n",
    "        # Get original points\n",
    "        data = read_ply(file_path)\n",
    "        return np.vstack((data['x'], data['y'], data['z'])).T\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Utility classes definition\n",
    "#       \\********************************/\n",
    "\n",
    "\n",
    "class S3DISSampler(Sampler):\n",
    "    \"\"\"Sampler for S3DIS\"\"\"\n",
    "\n",
    "    def __init__(self, dataset: S3DISDataset):\n",
    "        Sampler.__init__(self, dataset)\n",
    "\n",
    "        # Dataset used by the sampler (no copy is made in memory)\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # Number of step per epoch\n",
    "        if dataset.set == 'training':\n",
    "            self.N = dataset.config.epoch_steps\n",
    "        else:\n",
    "            self.N = dataset.config.validation_size\n",
    "\n",
    "        return\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yield next batch indices here. In this dataset, this is a dummy sampler that yield the index of batch element\n",
    "        (input sphere) in epoch instead of the list of point indices\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.dataset.use_potentials:\n",
    "\n",
    "            # Initiate current epoch ind\n",
    "            self.dataset.epoch_i *= 0\n",
    "            self.dataset.epoch_inds *= 0\n",
    "\n",
    "            # Initiate container for indices\n",
    "            all_epoch_inds = np.zeros((2, 0), dtype=np.int64)\n",
    "\n",
    "            # Number of sphere centers taken per class in each cloud\n",
    "            num_centers = self.N * self.dataset.config.batch_num\n",
    "            random_pick_n = int(np.ceil(num_centers / self.dataset.config.num_classes))\n",
    "\n",
    "            # Choose random points of each class for each cloud\n",
    "            epoch_indices = np.zeros((2, 0), dtype=np.int64)\n",
    "            for label_ind, label in enumerate(self.dataset.label_values):\n",
    "                if label not in self.dataset.ignored_labels:\n",
    "\n",
    "                    # Gather indices of the points with this label in all the input clouds \n",
    "                    all_label_indices = []\n",
    "                    for cloud_ind, cloud_labels in enumerate(self.dataset.input_labels):\n",
    "                        label_indices = np.where(np.equal(cloud_labels, label))[0]\n",
    "                        all_label_indices.append(np.vstack((np.full(label_indices.shape, cloud_ind, dtype=np.int64), label_indices)))\n",
    "\n",
    "                    # Stack them: [2, N1+N2+...]\n",
    "                    all_label_indices = np.hstack(all_label_indices)\n",
    "\n",
    "                    # Select a a random number amongst them\n",
    "                    N_inds = all_label_indices.shape[1]\n",
    "                    if N_inds < random_pick_n:\n",
    "                        chosen_label_inds = np.zeros((2, 0), dtype=np.int64)\n",
    "                        while chosen_label_inds.shape[1] < random_pick_n:\n",
    "                            chosen_label_inds = np.hstack((chosen_label_inds, all_label_indices[:, np.random.permutation(N_inds)]))\n",
    "                        warnings.warn('When choosing random epoch indices (use_potentials=False), \\\n",
    "                                       class {:d}: {:s} only had {:d} available points, while we \\\n",
    "                                       needed {:d}. Repeating indices in the same epoch'.format(label,\n",
    "                                                                                                self.dataset.label_names[label_ind],\n",
    "                                                                                                N_inds,\n",
    "                                                                                                random_pick_n))\n",
    "\n",
    "                    elif N_inds < 50 * random_pick_n:\n",
    "                        rand_inds = np.random.choice(N_inds, size=random_pick_n, replace=False)\n",
    "                        chosen_label_inds = all_label_indices[:, rand_inds]\n",
    "\n",
    "                    else:\n",
    "                        chosen_label_inds = np.zeros((2, 0), dtype=np.int64)\n",
    "                        while chosen_label_inds.shape[1] < random_pick_n:\n",
    "                            rand_inds = np.unique(np.random.choice(N_inds, size=2*random_pick_n, replace=True))\n",
    "                            chosen_label_inds = np.hstack((chosen_label_inds, all_label_indices[:, rand_inds]))\n",
    "                        chosen_label_inds = chosen_label_inds[:, :random_pick_n]\n",
    "\n",
    "                    # Stack for each label\n",
    "                    all_epoch_inds = np.hstack((all_epoch_inds, chosen_label_inds))\n",
    "\n",
    "            # Random permutation of the indices\n",
    "            random_order = np.random.permutation(all_epoch_inds.shape[1])[:num_centers]\n",
    "            all_epoch_inds = all_epoch_inds[:, random_order].astype(np.int64)\n",
    "\n",
    "            # Update epoch inds\n",
    "            self.dataset.epoch_inds += torch.from_numpy(all_epoch_inds)\n",
    "\n",
    "        # Generator loop\n",
    "        for i in range(self.N):\n",
    "            yield i\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The number of yielded samples is variable\n",
    "        \"\"\"\n",
    "        return self.N\n",
    "\n",
    "    def fast_calib(self):\n",
    "        \"\"\"\n",
    "        This method calibrates the batch sizes while ensuring the potentials are well initialized. Indeed on a dataset\n",
    "        like Semantic3D, before potential have been updated over the dataset, there are cahnces that all the dense area\n",
    "        are picked in the begining and in the end, we will have very large batch of small point clouds\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Estimated average batch size and target value\n",
    "        estim_b = 0\n",
    "        target_b = self.dataset.config.batch_num\n",
    "\n",
    "        # Calibration parameters\n",
    "        low_pass_T = 10\n",
    "        Kp = 100.0\n",
    "        finer = False\n",
    "        breaking = False\n",
    "\n",
    "        # Convergence parameters\n",
    "        smooth_errors = []\n",
    "        converge_threshold = 0.1\n",
    "\n",
    "        t = [time.time()]\n",
    "        last_display = time.time()\n",
    "        mean_dt = np.zeros(2)\n",
    "\n",
    "        for epoch in range(10):\n",
    "            for i, test in enumerate(self):\n",
    "\n",
    "                # New time\n",
    "                t = t[-1:]\n",
    "                t += [time.time()]\n",
    "\n",
    "                # batch length\n",
    "                b = len(test)\n",
    "\n",
    "                # Update estim_b (low pass filter)\n",
    "                estim_b += (b - estim_b) / low_pass_T\n",
    "\n",
    "                # Estimate error (noisy)\n",
    "                error = target_b - b\n",
    "\n",
    "                # Save smooth errors for convergene check\n",
    "                smooth_errors.append(target_b - estim_b)\n",
    "                if len(smooth_errors) > 10:\n",
    "                    smooth_errors = smooth_errors[1:]\n",
    "\n",
    "                # Update batch limit with P controller\n",
    "                self.dataset.batch_limit += Kp * error\n",
    "\n",
    "                # finer low pass filter when closing in\n",
    "                if not finer and np.abs(estim_b - target_b) < 1:\n",
    "                    low_pass_T = 100\n",
    "                    finer = True\n",
    "\n",
    "                # Convergence\n",
    "                if finer and np.max(np.abs(smooth_errors)) < converge_threshold:\n",
    "                    breaking = True\n",
    "                    break\n",
    "\n",
    "                # Average timing\n",
    "                t += [time.time()]\n",
    "                mean_dt = 0.9 * mean_dt + 0.1 * (np.array(t[1:]) - np.array(t[:-1]))\n",
    "\n",
    "                # Console display (only one per second)\n",
    "                if (t[-1] - last_display) > 1.0:\n",
    "                    last_display = t[-1]\n",
    "                    message = 'Step {:5d}  estim_b ={:5.2f} batch_limit ={:7d},  //  {:.1f}ms {:.1f}ms'\n",
    "                    print(message.format(i,\n",
    "                                         estim_b,\n",
    "                                         int(self.dataset.batch_limit),\n",
    "                                         1000 * mean_dt[0],\n",
    "                                         1000 * mean_dt[1]))\n",
    "\n",
    "            if breaking:\n",
    "                break\n",
    "\n",
    "    def calibration(self, dataloader, untouched_ratio=0.9, verbose=False, force_redo=False):\n",
    "        \"\"\"\n",
    "        Method performing batch and neighbors calibration.\n",
    "            Batch calibration: Set \"batch_limit\" (the maximum number of points allowed in every batch) so that the\n",
    "                               average batch size (number of stacked pointclouds) is the one asked.\n",
    "        Neighbors calibration: Set the \"neighborhood_limits\" (the maximum number of neighbors allowed in convolutions)\n",
    "                               so that 90% of the neighborhoods remain untouched. There is a limit for each layer.\n",
    "        \"\"\"\n",
    "\n",
    "        ##############################\n",
    "        # Previously saved calibration\n",
    "        ##############################\n",
    "\n",
    "        print('\\nStarting Calibration (use verbose=True for more details)')\n",
    "        t0 = time.time()\n",
    "\n",
    "        redo = force_redo\n",
    "\n",
    "        # Batch limit\n",
    "        # ***********\n",
    "\n",
    "        # Load batch_limit dictionary\n",
    "        batch_lim_file = join(self.dataset.path, 'batch_limits.pkl')\n",
    "        if exists(batch_lim_file):\n",
    "            with open(batch_lim_file, 'rb') as file:\n",
    "                batch_lim_dict = pickle.load(file)\n",
    "        else:\n",
    "            batch_lim_dict = {}\n",
    "\n",
    "        # Check if the batch limit associated with current parameters exists\n",
    "        if self.dataset.use_potentials:\n",
    "            sampler_method = 'potentials'\n",
    "        else:\n",
    "            sampler_method = 'random'\n",
    "        key = '{:s}_{:.3f}_{:.3f}_{:d}'.format(sampler_method,\n",
    "                                               self.dataset.config.in_radius,\n",
    "                                               self.dataset.config.first_subsampling_dl,\n",
    "                                               self.dataset.config.batch_num)\n",
    "        if not redo and key in batch_lim_dict:\n",
    "            self.dataset.batch_limit[0] = batch_lim_dict[key]\n",
    "        else:\n",
    "            redo = True\n",
    "\n",
    "        if verbose:\n",
    "            print('\\nPrevious calibration found:')\n",
    "            print('Check batch limit dictionary')\n",
    "            if key in batch_lim_dict:\n",
    "                color = bcolors.OKGREEN\n",
    "                v = str(int(batch_lim_dict[key]))\n",
    "            else:\n",
    "                color = bcolors.FAIL\n",
    "                v = '?'\n",
    "            print('{:}\\\"{:s}\\\": {:s}{:}'.format(color, key, v, bcolors.ENDC))\n",
    "\n",
    "        # Neighbors limit\n",
    "        # ***************\n",
    "\n",
    "        # Load neighb_limits dictionary\n",
    "        neighb_lim_file = join(self.dataset.path, 'neighbors_limits.pkl')\n",
    "        if exists(neighb_lim_file):\n",
    "            with open(neighb_lim_file, 'rb') as file:\n",
    "                neighb_lim_dict = pickle.load(file)\n",
    "        else:\n",
    "            neighb_lim_dict = {}\n",
    "\n",
    "        # Check if the limit associated with current parameters exists (for each layer)\n",
    "        neighb_limits = []\n",
    "        for layer_ind in range(self.dataset.config.num_layers):\n",
    "\n",
    "            dl = self.dataset.config.first_subsampling_dl * (2**layer_ind)\n",
    "            if self.dataset.config.deform_layers[layer_ind]:\n",
    "                r = dl * self.dataset.config.deform_radius\n",
    "            else:\n",
    "                r = dl * self.dataset.config.conv_radius\n",
    "\n",
    "            key = '{:.3f}_{:.3f}'.format(dl, r)\n",
    "            if key in neighb_lim_dict:\n",
    "                neighb_limits += [neighb_lim_dict[key]]\n",
    "\n",
    "        if not redo and len(neighb_limits) == self.dataset.config.num_layers:\n",
    "            self.dataset.neighborhood_limits = neighb_limits\n",
    "        else:\n",
    "            redo = True\n",
    "\n",
    "        if verbose:\n",
    "            print('Check neighbors limit dictionary')\n",
    "            for layer_ind in range(self.dataset.config.num_layers):\n",
    "                dl = self.dataset.config.first_subsampling_dl * (2**layer_ind)\n",
    "                if self.dataset.config.deform_layers[layer_ind]:\n",
    "                    r = dl * self.dataset.config.deform_radius\n",
    "                else:\n",
    "                    r = dl * self.dataset.config.conv_radius\n",
    "                key = '{:.3f}_{:.3f}'.format(dl, r)\n",
    "\n",
    "                if key in neighb_lim_dict:\n",
    "                    color = bcolors.OKGREEN\n",
    "                    v = str(neighb_lim_dict[key])\n",
    "                else:\n",
    "                    color = bcolors.FAIL\n",
    "                    v = '?'\n",
    "                print('{:}\\\"{:s}\\\": {:s}{:}'.format(color, key, v, bcolors.ENDC))\n",
    "\n",
    "        if redo:\n",
    "\n",
    "            ############################\n",
    "            # Neighbors calib parameters\n",
    "            ############################\n",
    "\n",
    "            # From config parameter, compute higher bound of neighbors number in a neighborhood\n",
    "            hist_n = int(np.ceil(4 / 3 * np.pi * (self.dataset.config.deform_radius + 1) ** 3))\n",
    "\n",
    "            # Histogram of neighborhood sizes\n",
    "            neighb_hists = np.zeros((self.dataset.config.num_layers, hist_n), dtype=np.int32)\n",
    "\n",
    "            ########################\n",
    "            # Batch calib parameters\n",
    "            ########################\n",
    "\n",
    "            # Estimated average batch size and target value\n",
    "            estim_b = 0\n",
    "            target_b = self.dataset.config.batch_num\n",
    "            \n",
    "            # Expected batch size order of magnitude\n",
    "            expected_N = 100000\n",
    "\n",
    "            # Calibration parameters. Higher means faster but can also become unstable\n",
    "            # Reduce Kp and Kd if your GP Uis small as the total number of points per batch will be smaller \n",
    "            low_pass_T = 100\n",
    "            Kp = expected_N / 200\n",
    "            Ki = 0.001 * Kp\n",
    "            Kd = 5 * Kp\n",
    "            finer = False\n",
    "            stabilized = False\n",
    "\n",
    "            # Convergence parameters\n",
    "            smooth_errors = []\n",
    "            converge_threshold = 0.1\n",
    "\n",
    "            # Loop parameters\n",
    "            last_display = time.time()\n",
    "            i = 0\n",
    "            breaking = False\n",
    "            error_I = 0\n",
    "            error_D = 0\n",
    "            last_error = 0\n",
    "\n",
    "            debug_in = []\n",
    "            debug_out = []\n",
    "            debug_b = []\n",
    "            debug_estim_b = []\n",
    "\n",
    "            #####################\n",
    "            # Perform calibration\n",
    "            #####################\n",
    "\n",
    "            # number of batch per epoch \n",
    "            sample_batches = 999\n",
    "            for epoch in range((sample_batches // self.N) + 1):\n",
    "                for batch_i, batch in enumerate(dataloader):\n",
    "\n",
    "                    # Update neighborhood histogram\n",
    "                    counts = [np.sum(neighb_mat.numpy() < neighb_mat.shape[0], axis=1) for neighb_mat in batch.neighbors]\n",
    "                    hists = [np.bincount(c, minlength=hist_n)[:hist_n] for c in counts]\n",
    "                    neighb_hists += np.vstack(hists)\n",
    "\n",
    "                    # batch length\n",
    "                    b = len(batch.cloud_inds)\n",
    "\n",
    "                    # Update estim_b (low pass filter)\n",
    "                    estim_b += (b - estim_b) / low_pass_T\n",
    "\n",
    "                    # Estimate error (noisy)\n",
    "                    error = target_b - b\n",
    "                    error_I += error\n",
    "                    error_D = error - last_error\n",
    "                    last_error = error\n",
    "\n",
    "\n",
    "                    # Save smooth errors for convergene check\n",
    "                    smooth_errors.append(target_b - estim_b)\n",
    "                    if len(smooth_errors) > 30:\n",
    "                        smooth_errors = smooth_errors[1:]\n",
    "\n",
    "                    # Update batch limit with P controller\n",
    "                    self.dataset.batch_limit += Kp * error + Ki * error_I + Kd * error_D\n",
    "\n",
    "                    # Unstability detection\n",
    "                    if not stabilized and self.dataset.batch_limit < 0:\n",
    "                        Kp *= 0.1\n",
    "                        Ki *= 0.1\n",
    "                        Kd *= 0.1\n",
    "                        stabilized = True\n",
    "\n",
    "                    # finer low pass filter when closing in\n",
    "                    if not finer and np.abs(estim_b - target_b) < 1:\n",
    "                        low_pass_T = 100\n",
    "                        finer = True\n",
    "\n",
    "                    # Convergence\n",
    "                    if finer and np.max(np.abs(smooth_errors)) < converge_threshold:\n",
    "                        breaking = True\n",
    "                        break\n",
    "\n",
    "                    i += 1\n",
    "                    t = time.time()\n",
    "\n",
    "                    # Console display (only one per second)\n",
    "                    if verbose and (t - last_display) > 1.0:\n",
    "                        last_display = t\n",
    "                        message = 'Step {:5d}  estim_b ={:5.2f} batch_limit ={:7d}'\n",
    "                        print(message.format(i,\n",
    "                                             estim_b,\n",
    "                                             int(self.dataset.batch_limit)))\n",
    "\n",
    "                    # Debug plots\n",
    "                    debug_in.append(int(batch.points[0].shape[0]))\n",
    "                    debug_out.append(int(self.dataset.batch_limit))\n",
    "                    debug_b.append(b)\n",
    "                    debug_estim_b.append(estim_b)\n",
    "\n",
    "                if breaking:\n",
    "                    break\n",
    "\n",
    "            # Plot in case we did not reach convergence\n",
    "            if not breaking:\n",
    "                import matplotlib.pyplot as plt\n",
    "\n",
    "                print(\"ERROR: It seems that the calibration have not reached convergence. Here are some plot to understand why:\")\n",
    "                print(\"If you notice unstability, reduce the expected_N value\")\n",
    "                print(\"If convergece is too slow, increase the expected_N value\")\n",
    "\n",
    "                plt.figure()\n",
    "                plt.plot(debug_in)\n",
    "                plt.plot(debug_out)\n",
    "\n",
    "                plt.figure()\n",
    "                plt.plot(debug_b)\n",
    "                plt.plot(debug_estim_b)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "                a = 1/0\n",
    "\n",
    "\n",
    "            # Use collected neighbor histogram to get neighbors limit\n",
    "            cumsum = np.cumsum(neighb_hists.T, axis=0)\n",
    "            percentiles = np.sum(cumsum < (untouched_ratio * cumsum[hist_n - 1, :]), axis=0)\n",
    "            self.dataset.neighborhood_limits = percentiles\n",
    "\n",
    "\n",
    "            if verbose:\n",
    "\n",
    "                # Crop histogram\n",
    "                while np.sum(neighb_hists[:, -1]) == 0:\n",
    "                    neighb_hists = neighb_hists[:, :-1]\n",
    "                hist_n = neighb_hists.shape[1]\n",
    "\n",
    "                print('\\n**************************************************\\n')\n",
    "                line0 = 'neighbors_num '\n",
    "                for layer in range(neighb_hists.shape[0]):\n",
    "                    line0 += '|  layer {:2d}  '.format(layer)\n",
    "                print(line0)\n",
    "                for neighb_size in range(hist_n):\n",
    "                    line0 = '     {:4d}     '.format(neighb_size)\n",
    "                    for layer in range(neighb_hists.shape[0]):\n",
    "                        if neighb_size > percentiles[layer]:\n",
    "                            color = bcolors.FAIL\n",
    "                        else:\n",
    "                            color = bcolors.OKGREEN\n",
    "                        line0 += '|{:}{:10d}{:}  '.format(color,\n",
    "                                                         neighb_hists[layer, neighb_size],\n",
    "                                                         bcolors.ENDC)\n",
    "\n",
    "                    print(line0)\n",
    "\n",
    "                print('\\n**************************************************\\n')\n",
    "                print('\\nchosen neighbors limits: ', percentiles)\n",
    "                print()\n",
    "\n",
    "            # Save batch_limit dictionary\n",
    "            if self.dataset.use_potentials:\n",
    "                sampler_method = 'potentials'\n",
    "            else:\n",
    "                sampler_method = 'random'\n",
    "            key = '{:s}_{:.3f}_{:.3f}_{:d}'.format(sampler_method,\n",
    "                                                   self.dataset.config.in_radius,\n",
    "                                                   self.dataset.config.first_subsampling_dl,\n",
    "                                                   self.dataset.config.batch_num)\n",
    "            batch_lim_dict[key] = float(self.dataset.batch_limit)\n",
    "            with open(batch_lim_file, 'wb') as file:\n",
    "                pickle.dump(batch_lim_dict, file)\n",
    "\n",
    "            # Save neighb_limit dictionary\n",
    "            for layer_ind in range(self.dataset.config.num_layers):\n",
    "                dl = self.dataset.config.first_subsampling_dl * (2 ** layer_ind)\n",
    "                if self.dataset.config.deform_layers[layer_ind]:\n",
    "                    r = dl * self.dataset.config.deform_radius\n",
    "                else:\n",
    "                    r = dl * self.dataset.config.conv_radius\n",
    "                key = '{:.3f}_{:.3f}'.format(dl, r)\n",
    "                neighb_lim_dict[key] = self.dataset.neighborhood_limits[layer_ind]\n",
    "            with open(neighb_lim_file, 'wb') as file:\n",
    "                pickle.dump(neighb_lim_dict, file)\n",
    "\n",
    "\n",
    "        print('Calibration done in {:.1f}s\\n'.format(time.time() - t0))\n",
    "        return\n",
    "\n",
    "\n",
    "class S3DISCustomBatch:\n",
    "    \"\"\"Custom batch definition with memory pinning for S3DIS\"\"\"\n",
    "\n",
    "    def __init__(self, input_list):\n",
    "\n",
    "        # Get rid of batch dimension\n",
    "        input_list = input_list[0]\n",
    "\n",
    "        # Number of layers\n",
    "        L = (len(input_list) - 7) // 5\n",
    "\n",
    "        # Extract input tensors from the list of numpy array\n",
    "        ind = 0\n",
    "        self.points = [torch.from_numpy(nparray) for nparray in input_list[ind:ind+L]]\n",
    "        ind += L\n",
    "        self.neighbors = [torch.from_numpy(nparray) for nparray in input_list[ind:ind+L]]\n",
    "        ind += L\n",
    "        self.pools = [torch.from_numpy(nparray) for nparray in input_list[ind:ind+L]]\n",
    "        ind += L\n",
    "        self.upsamples = [torch.from_numpy(nparray) for nparray in input_list[ind:ind+L]]\n",
    "        ind += L\n",
    "        self.lengths = [torch.from_numpy(nparray) for nparray in input_list[ind:ind+L]]\n",
    "        ind += L\n",
    "        self.features = torch.from_numpy(input_list[ind])\n",
    "        ind += 1\n",
    "        self.labels = torch.from_numpy(input_list[ind])\n",
    "        ind += 1\n",
    "        self.scales = torch.from_numpy(input_list[ind])\n",
    "        ind += 1\n",
    "        self.rots = torch.from_numpy(input_list[ind])\n",
    "        ind += 1\n",
    "        self.cloud_inds = torch.from_numpy(input_list[ind])\n",
    "        ind += 1\n",
    "        self.center_inds = torch.from_numpy(input_list[ind])\n",
    "        ind += 1\n",
    "        self.input_inds = torch.from_numpy(input_list[ind])\n",
    "\n",
    "        return\n",
    "\n",
    "    def pin_memory(self):\n",
    "        \"\"\"\n",
    "        Manual pinning of the memory\n",
    "        \"\"\"\n",
    "\n",
    "        self.points = [in_tensor.pin_memory() for in_tensor in self.points]\n",
    "        self.neighbors = [in_tensor.pin_memory() for in_tensor in self.neighbors]\n",
    "        self.pools = [in_tensor.pin_memory() for in_tensor in self.pools]\n",
    "        self.upsamples = [in_tensor.pin_memory() for in_tensor in self.upsamples]\n",
    "        self.lengths = [in_tensor.pin_memory() for in_tensor in self.lengths]\n",
    "        self.features = self.features.pin_memory()\n",
    "        self.labels = self.labels.pin_memory()\n",
    "        self.scales = self.scales.pin_memory()\n",
    "        self.rots = self.rots.pin_memory()\n",
    "        self.cloud_inds = self.cloud_inds.pin_memory()\n",
    "        self.center_inds = self.center_inds.pin_memory()\n",
    "        self.input_inds = self.input_inds.pin_memory()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def to(self, device):\n",
    "\n",
    "        self.points = [in_tensor.to(device) for in_tensor in self.points]\n",
    "        self.neighbors = [in_tensor.to(device) for in_tensor in self.neighbors]\n",
    "        self.pools = [in_tensor.to(device) for in_tensor in self.pools]\n",
    "        self.upsamples = [in_tensor.to(device) for in_tensor in self.upsamples]\n",
    "        self.lengths = [in_tensor.to(device) for in_tensor in self.lengths]\n",
    "        self.features = self.features.to(device)\n",
    "        self.labels = self.labels.to(device)\n",
    "        self.scales = self.scales.to(device)\n",
    "        self.rots = self.rots.to(device)\n",
    "        self.cloud_inds = self.cloud_inds.to(device)\n",
    "        self.center_inds = self.center_inds.to(device)\n",
    "        self.input_inds = self.input_inds.to(device)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def unstack_points(self, layer=None):\n",
    "        \"\"\"Unstack the points\"\"\"\n",
    "        return self.unstack_elements('points', layer)\n",
    "\n",
    "    def unstack_neighbors(self, layer=None):\n",
    "        \"\"\"Unstack the neighbors indices\"\"\"\n",
    "        return self.unstack_elements('neighbors', layer)\n",
    "\n",
    "    def unstack_pools(self, layer=None):\n",
    "        \"\"\"Unstack the pooling indices\"\"\"\n",
    "        return self.unstack_elements('pools', layer)\n",
    "\n",
    "    def unstack_elements(self, element_name, layer=None, to_numpy=True):\n",
    "        \"\"\"\n",
    "        Return a list of the stacked elements in the batch at a certain layer. If no layer is given, then return all\n",
    "        layers\n",
    "        \"\"\"\n",
    "\n",
    "        if element_name == 'points':\n",
    "            elements = self.points\n",
    "        elif element_name == 'neighbors':\n",
    "            elements = self.neighbors\n",
    "        elif element_name == 'pools':\n",
    "            elements = self.pools[:-1]\n",
    "        else:\n",
    "            raise ValueError('Unknown element name: {:s}'.format(element_name))\n",
    "\n",
    "        all_p_list = []\n",
    "        for layer_i, layer_elems in enumerate(elements):\n",
    "\n",
    "            if layer is None or layer == layer_i:\n",
    "\n",
    "                i0 = 0\n",
    "                p_list = []\n",
    "                if element_name == 'pools':\n",
    "                    lengths = self.lengths[layer_i+1]\n",
    "                else:\n",
    "                    lengths = self.lengths[layer_i]\n",
    "\n",
    "                for b_i, length in enumerate(lengths):\n",
    "\n",
    "                    elem = layer_elems[i0:i0 + length]\n",
    "                    if element_name == 'neighbors':\n",
    "                        elem[elem >= self.points[layer_i].shape[0]] = -1\n",
    "                        elem[elem >= 0] -= i0\n",
    "                    elif element_name == 'pools':\n",
    "                        elem[elem >= self.points[layer_i].shape[0]] = -1\n",
    "                        elem[elem >= 0] -= torch.sum(self.lengths[layer_i][:b_i])\n",
    "                    i0 += length\n",
    "\n",
    "                    if to_numpy:\n",
    "                        p_list.append(elem.numpy())\n",
    "                    else:\n",
    "                        p_list.append(elem)\n",
    "\n",
    "                if layer == layer_i:\n",
    "                    return p_list\n",
    "\n",
    "                all_p_list.append(p_list)\n",
    "\n",
    "        return all_p_list\n",
    "\n",
    "\n",
    "def S3DISCollate(batch_data):\n",
    "    return S3DISCustomBatch(batch_data)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Debug functions\n",
    "#       \\*********************/\n",
    "\n",
    "\n",
    "def debug_upsampling(dataset, loader):\n",
    "    \"\"\"Shows which labels are sampled according to strategy chosen\"\"\"\n",
    "\n",
    "\n",
    "    for epoch in range(10):\n",
    "\n",
    "        for batch_i, batch in enumerate(loader):\n",
    "\n",
    "            pc1 = batch.points[1].numpy()\n",
    "            pc2 = batch.points[2].numpy()\n",
    "            up1 = batch.upsamples[1].numpy()\n",
    "\n",
    "            print(pc1.shape, '=>', pc2.shape)\n",
    "            print(up1.shape, np.max(up1))\n",
    "\n",
    "            pc2 = np.vstack((pc2, np.zeros_like(pc2[:1, :])))\n",
    "\n",
    "            # Get neighbors distance\n",
    "            p0 = pc1[10, :]\n",
    "            neighbs0 = up1[10, :]\n",
    "            neighbs0 = pc2[neighbs0, :] - p0\n",
    "            d2 = np.sum(neighbs0 ** 2, axis=1)\n",
    "\n",
    "            print(neighbs0.shape)\n",
    "            print(neighbs0[:5])\n",
    "            print(d2[:5])\n",
    "\n",
    "            print('******************')\n",
    "        print('*******************************************')\n",
    "\n",
    "    _, counts = np.unique(dataset.input_labels, return_counts=True)\n",
    "    print(counts)\n",
    "\n",
    "\n",
    "def debug_timing(dataset, loader):\n",
    "    \"\"\"Timing of generator function\"\"\"\n",
    "\n",
    "    t = [time.time()]\n",
    "    last_display = time.time()\n",
    "    mean_dt = np.zeros(2)\n",
    "    estim_b = dataset.config.batch_num\n",
    "    estim_N = 0\n",
    "\n",
    "    for epoch in range(10):\n",
    "\n",
    "        for batch_i, batch in enumerate(loader):\n",
    "            # print(batch_i, tuple(points.shape),  tuple(normals.shape), labels, indices, in_sizes)\n",
    "\n",
    "            # New time\n",
    "            t = t[-1:]\n",
    "            t += [time.time()]\n",
    "\n",
    "            # Update estim_b (low pass filter)\n",
    "            estim_b += (len(batch.cloud_inds) - estim_b) / 100\n",
    "            estim_N += (batch.features.shape[0] - estim_N) / 10\n",
    "\n",
    "            # Pause simulating computations\n",
    "            time.sleep(0.05)\n",
    "            t += [time.time()]\n",
    "\n",
    "            # Average timing\n",
    "            mean_dt = 0.9 * mean_dt + 0.1 * (np.array(t[1:]) - np.array(t[:-1]))\n",
    "\n",
    "            # Console display (only one per second)\n",
    "            if (t[-1] - last_display) > -1.0:\n",
    "                last_display = t[-1]\n",
    "                message = 'Step {:08d} -> (ms/batch) {:8.2f} {:8.2f} / batch = {:.2f} - {:.0f}'\n",
    "                print(message.format(batch_i,\n",
    "                                     1000 * mean_dt[0],\n",
    "                                     1000 * mean_dt[1],\n",
    "                                     estim_b,\n",
    "                                     estim_N))\n",
    "\n",
    "        print('************* Epoch ended *************')\n",
    "\n",
    "    _, counts = np.unique(dataset.input_labels, return_counts=True)\n",
    "    print(counts)\n",
    "\n",
    "\n",
    "def debug_show_clouds(dataset, loader):\n",
    "\n",
    "\n",
    "    for epoch in range(10):\n",
    "\n",
    "        clouds = []\n",
    "        cloud_normals = []\n",
    "        cloud_labels = []\n",
    "\n",
    "        L = dataset.config.num_layers\n",
    "\n",
    "        for batch_i, batch in enumerate(loader):\n",
    "\n",
    "            # Print characteristics of input tensors\n",
    "            print('\\nPoints tensors')\n",
    "            for i in range(L):\n",
    "                print(batch.points[i].dtype, batch.points[i].shape)\n",
    "            print('\\nNeigbors tensors')\n",
    "            for i in range(L):\n",
    "                print(batch.neighbors[i].dtype, batch.neighbors[i].shape)\n",
    "            print('\\nPools tensors')\n",
    "            for i in range(L):\n",
    "                print(batch.pools[i].dtype, batch.pools[i].shape)\n",
    "            print('\\nStack lengths')\n",
    "            for i in range(L):\n",
    "                print(batch.lengths[i].dtype, batch.lengths[i].shape)\n",
    "            print('\\nFeatures')\n",
    "            print(batch.features.dtype, batch.features.shape)\n",
    "            print('\\nLabels')\n",
    "            print(batch.labels.dtype, batch.labels.shape)\n",
    "            print('\\nAugment Scales')\n",
    "            print(batch.scales.dtype, batch.scales.shape)\n",
    "            print('\\nAugment Rotations')\n",
    "            print(batch.rots.dtype, batch.rots.shape)\n",
    "            print('\\nModel indices')\n",
    "            print(batch.model_inds.dtype, batch.model_inds.shape)\n",
    "\n",
    "            print('\\nAre input tensors pinned')\n",
    "            print(batch.neighbors[0].is_pinned())\n",
    "            print(batch.neighbors[-1].is_pinned())\n",
    "            print(batch.points[0].is_pinned())\n",
    "            print(batch.points[-1].is_pinned())\n",
    "            print(batch.labels.is_pinned())\n",
    "            print(batch.scales.is_pinned())\n",
    "            print(batch.rots.is_pinned())\n",
    "            print(batch.model_inds.is_pinned())\n",
    "\n",
    "            show_input_batch(batch)\n",
    "\n",
    "        print('*******************************************')\n",
    "\n",
    "    _, counts = np.unique(dataset.input_labels, return_counts=True)\n",
    "    print(counts)\n",
    "\n",
    "\n",
    "def debug_batch_and_neighbors_calib(dataset, loader):\n",
    "    \"\"\"Timing of generator function\"\"\"\n",
    "\n",
    "    t = [time.time()]\n",
    "    last_display = time.time()\n",
    "    mean_dt = np.zeros(2)\n",
    "\n",
    "    for epoch in range(10):\n",
    "\n",
    "        for batch_i, input_list in enumerate(loader):\n",
    "            # print(batch_i, tuple(points.shape),  tuple(normals.shape), labels, indices, in_sizes)\n",
    "\n",
    "            # New time\n",
    "            t = t[-1:]\n",
    "            t += [time.time()]\n",
    "\n",
    "            # Pause simulating computations\n",
    "            time.sleep(0.01)\n",
    "            t += [time.time()]\n",
    "\n",
    "            # Average timing\n",
    "            mean_dt = 0.9 * mean_dt + 0.1 * (np.array(t[1:]) - np.array(t[:-1]))\n",
    "\n",
    "            # Console display (only one per second)\n",
    "            if (t[-1] - last_display) > 1.0:\n",
    "                last_display = t[-1]\n",
    "                message = 'Step {:08d} -> Average timings (ms/batch) {:8.2f} {:8.2f} '\n",
    "                print(message.format(batch_i,\n",
    "                                     1000 * mean_dt[0],\n",
    "                                     1000 * mean_dt[1]))\n",
    "\n",
    "        print('************* Epoch ended *************')\n",
    "\n",
    "    _, counts = np.unique(dataset.input_labels, return_counts=True)\n",
    "    print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(m, i, o):\n",
    "    print(m)\n",
    "    try:\n",
    "        print(m.block_name)\n",
    "        resnet_encodings.append(o[0].detach().numpy())\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "    print(\"================================\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config = Config()\n",
    "config.load(os.path.join(target_dir, \"Heavy_KPFCNN\"))\n",
    "checkpoint = torch.load(os.path.join(target_dir, \"Heavy_KPFCNN\", \"checkpoints\", \"chkp_0500.tar\"), map_location=device)\n",
    "net = KPFCNN(config, np.arange(0, 13), np.empty((0,)))\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "net.eval()\n",
    "\n",
    "resnet_encodings = []\n",
    "for block in net.encoder_blocks:\n",
    "    block.register_forward_hook(hook_fn)\n",
    "for block in net.decoder_blocks:\n",
    "    block.register_forward_hook(hook_fn)\n",
    "net.head_mlp.register_forward_hook(hook_fn)\n",
    "net.head_softmax.register_forward_hook(hook_fn)\n",
    "net.l1.register_forward_hook(hook_fn)\n",
    "\n",
    "point_clouds_path = []\n",
    "for dirpath, dirnames, filenames in os.walk(os.path.join(target_dir, input_dir)):\n",
    "    point_clouds_path.extend(filenames)\n",
    "    break\n",
    "point_clouds_id = {}\n",
    "for point_cloud_path in point_clouds_path:\n",
    "    point_clouds_id['.'.join(point_cloud_path.split('.')[:-1])] = os.path.join(target_dir, input_dir, point_cloud_path)\n",
    "\n",
    "# Path(os.path.join(target_dir, output_dir)).mkdir(parents=True, exist_ok=True)\n",
    "# for id, path in point_clouds_id.items():\n",
    "#     point_cloud = []\n",
    "#     with open(path, 'r') as fp:\n",
    "#         csv_reader = csv.reader(fp)\n",
    "#         for row in csv_reader:\n",
    "#             point_cloud.append([float(i) for i in row])\n",
    "#     if len(point_cloud) <= 0:\n",
    "#         continue\n",
    "#     print(point_cloud[0])\n",
    "#     point_cloud_encoded = feature_extractor(np.array(point_cloud)).tolist()\n",
    "#     with open(os.path.join(target_dir, output_dir, id + \".json\"), 'w') as fp:\n",
    "#         json.dump(point_cloud_encoded, fp)\n",
    "\n",
    "\n",
    "test_dataset = S3DISDataset(config, path=os.path.join(target_dir, input_dir), cloud_names=[\"testid\"], set=\"validation\", use_potentials=True)\n",
    "test_sampler = S3DISSampler(test_dataset)\n",
    "collate_fn = S3DISCollate\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, num_workers=config.input_threads, pin_memory=True)\n",
    "\n",
    "test_batch = None\n",
    "for i, batch in enumerate(test_loader):\n",
    "    test_batch = batch\n",
    "    break\n",
    "output = net(test_batch, config)\n",
    "shutil.rmtree(os.path.join(target_dir, input_dir, \"original_ply\"))\n",
    "shutil.rmtree(os.path.join(target_dir, input_dir, \"input_0.030\"))\n",
    "\n",
    "resnet_encoding = resnet_encodings[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
