{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Dockerfiles have not been modified, connect to the Jupyter server with:  \n",
    "- ```http://localhost:8013/tree?token=encode-segmented-point-clouds-cpu``` to run Torch on CPU\n",
    "\n",
    "Before building the docker image, ensure that ```chkp_0500.tar``` has been downloaded into [```encode-segmented-point-clouds-cpu```](/encode-segmented-point-clouds-cpu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"data\"\n",
    "checkpoint_file = \"chkp_0500.tar\"\n",
    "input_dir = \"place-pulse-singapore-point-clouds-512-1024-segmented\"\n",
    "output_dir = \"place-pulse-singapore-point-clouds-512-1024-encoded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPFCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Class defining KPFCNN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, lbl_values, ign_lbls):\n",
    "        super(KPFCNN, self).__init__()\n",
    "\n",
    "        ############\n",
    "        # Parameters\n",
    "        ############\n",
    "\n",
    "        # Current radius of convolution and feature dimension\n",
    "        layer = 0\n",
    "        r = config.first_subsampling_dl * config.conv_radius\n",
    "        in_dim = config.in_features_dim\n",
    "        out_dim = config.first_features_dim\n",
    "        self.K = config.num_kernel_points\n",
    "        self.C = len(lbl_values) - len(ign_lbls)\n",
    "\n",
    "        #####################\n",
    "        # List Encoder blocks\n",
    "        #####################\n",
    "\n",
    "        # Save all block operations in a list of modules\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        self.encoder_skip_dims = []\n",
    "        self.encoder_skips = []\n",
    "\n",
    "        # Loop over consecutive blocks\n",
    "        for block_i, block in enumerate(config.architecture):\n",
    "\n",
    "            # Check equivariance\n",
    "            if ('equivariant' in block) and (not out_dim % 3 == 0):\n",
    "                raise ValueError('Equivariant block but features dimension is not a factor of 3')\n",
    "\n",
    "            # Detect change to next layer for skip connection\n",
    "            if np.any([tmp in block for tmp in ['pool', 'strided', 'upsample', 'global']]):\n",
    "                self.encoder_skips.append(block_i)\n",
    "                self.encoder_skip_dims.append(in_dim)\n",
    "\n",
    "            # Detect upsampling block to stop\n",
    "            if 'upsample' in block:\n",
    "                break\n",
    "\n",
    "            # Apply the good block function defining tf ops\n",
    "            self.encoder_blocks.append(block_decider(block,\n",
    "                                                    r,\n",
    "                                                    in_dim,\n",
    "                                                    out_dim,\n",
    "                                                    layer,\n",
    "                                                    config))\n",
    "\n",
    "            # Update dimension of input from output\n",
    "            if 'simple' in block:\n",
    "                in_dim = out_dim // 2\n",
    "            else:\n",
    "                in_dim = out_dim\n",
    "\n",
    "            # Detect change to a subsampled layer\n",
    "            if 'pool' in block or 'strided' in block:\n",
    "                # Update radius and feature dimension for next layer\n",
    "                layer += 1\n",
    "                r *= 2\n",
    "                out_dim *= 2\n",
    "\n",
    "        #####################\n",
    "        # List Decoder blocks\n",
    "        #####################\n",
    "\n",
    "        # Save all block operations in a list of modules\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        self.decoder_concats = []\n",
    "\n",
    "        # Find first upsampling block\n",
    "        start_i = 0\n",
    "        for block_i, block in enumerate(config.architecture):\n",
    "            if 'upsample' in block:\n",
    "                start_i = block_i\n",
    "                break\n",
    "\n",
    "        # Loop over consecutive blocks\n",
    "        for block_i, block in enumerate(config.architecture[start_i:]):\n",
    "\n",
    "            # Add dimension of skip connection concat\n",
    "            if block_i > 0 and 'upsample' in config.architecture[start_i + block_i - 1]:\n",
    "                in_dim += self.encoder_skip_dims[layer]\n",
    "                self.decoder_concats.append(block_i)\n",
    "\n",
    "            # Apply the good block function defining tf ops\n",
    "            self.decoder_blocks.append(block_decider(block,\n",
    "                                                    r,\n",
    "                                                    in_dim,\n",
    "                                                    out_dim,\n",
    "                                                    layer,\n",
    "                                                    config))\n",
    "\n",
    "            # Update dimension of input from output\n",
    "            in_dim = out_dim\n",
    "\n",
    "            # Detect change to a subsampled layer\n",
    "            if 'upsample' in block:\n",
    "                # Update radius and feature dimension for next layer\n",
    "                layer -= 1\n",
    "                r *= 0.5\n",
    "                out_dim = out_dim // 2\n",
    "\n",
    "        self.head_mlp = UnaryBlock(out_dim, config.first_features_dim, False, 0)\n",
    "        self.head_softmax = UnaryBlock(config.first_features_dim, self.C, False, 0, no_relu=True)\n",
    "\n",
    "        ################\n",
    "        # Network Losses\n",
    "        ################\n",
    "\n",
    "        # List of valid labels (those not ignored in loss)\n",
    "        self.valid_labels = np.sort([c for c in lbl_values if c not in ign_lbls])\n",
    "\n",
    "        # Choose segmentation loss\n",
    "        if len(config.class_w) > 0:\n",
    "            class_w = torch.from_numpy(np.array(config.class_w, dtype=np.float32))\n",
    "            self.criterion = torch.nn.CrossEntropyLoss(weight=class_w, ignore_index=-1)\n",
    "        else:\n",
    "            self.criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        self.deform_fitting_mode = config.deform_fitting_mode\n",
    "        self.deform_fitting_power = config.deform_fitting_power\n",
    "        self.deform_lr_factor = config.deform_lr_factor\n",
    "        self.repulse_extent = config.repulse_extent\n",
    "        self.output_loss = 0\n",
    "        self.reg_loss = 0\n",
    "        self.l1 = nn.L1Loss()\n",
    "\n",
    "        return\n",
    "\n",
    "    def forward(self, batch, config):\n",
    "\n",
    "        # Get input features\n",
    "        x = batch.features.clone().detach()\n",
    "\n",
    "        # Loop over consecutive blocks\n",
    "        skip_x = []\n",
    "        for block_i, block_op in enumerate(self.encoder_blocks):\n",
    "            if block_i in self.encoder_skips:\n",
    "                skip_x.append(x)\n",
    "            x = block_op(x, batch)\n",
    "\n",
    "        for block_i, block_op in enumerate(self.decoder_blocks):\n",
    "            if block_i in self.decoder_concats:\n",
    "                x = torch.cat([x, skip_x.pop()], dim=1)\n",
    "            x = block_op(x, batch)\n",
    "\n",
    "        # Head of network\n",
    "        x = self.head_mlp(x, batch)\n",
    "        x = self.head_softmax(x, batch)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Runs the loss on outputs of the model\n",
    "        :param outputs: logits\n",
    "        :param labels: labels\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Set all ignored labels to -1 and correct the other label to be in [0, C-1] range\n",
    "        target = - torch.ones_like(labels)\n",
    "        for i, c in enumerate(self.valid_labels):\n",
    "            target[labels == c] = i\n",
    "\n",
    "        # Reshape to have a minibatch size of 1\n",
    "        outputs = torch.transpose(outputs, 0, 1)\n",
    "        outputs = outputs.unsqueeze(0)\n",
    "        target = target.unsqueeze(0)\n",
    "\n",
    "        # Cross entropy loss\n",
    "        self.output_loss = self.criterion(outputs, target)\n",
    "\n",
    "        # Regularization of deformable offsets\n",
    "        if self.deform_fitting_mode == 'point2point':\n",
    "            self.reg_loss = p2p_fitting_regularizer(self)\n",
    "        elif self.deform_fitting_mode == 'point2plane':\n",
    "            raise ValueError('point2plane fitting mode not implemented yet.')\n",
    "        else:\n",
    "            raise ValueError('Unknown fitting mode: ' + self.deform_fitting_mode)\n",
    "\n",
    "        # Combined loss\n",
    "        return self.output_loss + self.reg_loss\n",
    "\n",
    "    def accuracy(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Computes accuracy of the current batch\n",
    "        :param outputs: logits predicted by the network\n",
    "        :param labels: labels\n",
    "        :return: accuracy value\n",
    "        \"\"\"\n",
    "\n",
    "        # Set all ignored labels to -1 and correct the other label to be in [0, C-1] range\n",
    "        target = - torch.ones_like(labels)\n",
    "        for i, c in enumerate(self.valid_labels):\n",
    "            target[labels == c] = i\n",
    "\n",
    "        predicted = torch.argmax(outputs.data, dim=1)\n",
    "        total = target.size(0)\n",
    "        correct = (predicted == target).sum().item()\n",
    "\n",
    "        return correct / total\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Class containing the parameters you want to modify for this dataset\n",
    "    \"\"\"\n",
    "\n",
    "    ##################\n",
    "    # Input parameters\n",
    "    ##################\n",
    "\n",
    "    # Dataset name\n",
    "    dataset = ''\n",
    "\n",
    "    # Type of network model\n",
    "    dataset_task = ''\n",
    "\n",
    "    # Number of classes in the dataset\n",
    "    num_classes = 0\n",
    "\n",
    "    # Dimension of input points\n",
    "    in_points_dim = 3\n",
    "\n",
    "    # Dimension of input features\n",
    "    in_features_dim = 1\n",
    "\n",
    "    # Radius of the input sphere (ignored for models, only used for point clouds)\n",
    "    in_radius = 1.0\n",
    "\n",
    "    # Number of CPU threads for the input pipeline\n",
    "    input_threads = 8\n",
    "\n",
    "    ##################\n",
    "    # Model parameters\n",
    "    ##################\n",
    "\n",
    "    # Architecture definition. List of blocks\n",
    "    architecture = []\n",
    "\n",
    "    # Decide the mode of equivariance and invariance\n",
    "    equivar_mode = ''\n",
    "    invar_mode = ''\n",
    "\n",
    "    # Dimension of the first feature maps\n",
    "    first_features_dim = 64\n",
    "\n",
    "    # Batch normalization parameters\n",
    "    use_batch_norm = True\n",
    "    batch_norm_momentum = 0.99\n",
    "\n",
    "    # For segmentation models : ratio between the segmented area and the input area\n",
    "    segmentation_ratio = 1.0\n",
    "\n",
    "    ###################\n",
    "    # KPConv parameters\n",
    "    ###################\n",
    "\n",
    "    # Number of kernel points\n",
    "    num_kernel_points = 15\n",
    "\n",
    "    # Size of the first subsampling grid in meter\n",
    "    first_subsampling_dl = 0.02\n",
    "\n",
    "    # Radius of convolution in \"number grid cell\". (2.5 is the standard value)\n",
    "    conv_radius = 2.5\n",
    "\n",
    "    # Radius of deformable convolution in \"number grid cell\". Larger so that deformed kernel can spread out\n",
    "    deform_radius = 5.0\n",
    "\n",
    "    # Kernel point influence radius\n",
    "    KP_extent = 1.0\n",
    "\n",
    "    # Influence function when d < KP_extent. ('constant', 'linear', 'gaussian') When d > KP_extent, always zero\n",
    "    KP_influence = 'linear'\n",
    "\n",
    "    # Aggregation function of KPConv in ('closest', 'sum')\n",
    "    # Decide if you sum all kernel point influences, or if you only take the influence of the closest KP\n",
    "    aggregation_mode = 'sum'\n",
    "\n",
    "    # Fixed points in the kernel : 'none', 'center' or 'verticals'\n",
    "    fixed_kernel_points = 'center'\n",
    "\n",
    "    # Use modulateion in deformable convolutions\n",
    "    modulated = False\n",
    "\n",
    "    # For SLAM datasets like SemanticKitti number of frames used (minimum one)\n",
    "    n_frames = 1\n",
    "\n",
    "    # For SLAM datasets like SemanticKitti max number of point in input cloud + validation\n",
    "    max_in_points = 0\n",
    "    val_radius = 51.0\n",
    "    max_val_points = 50000\n",
    "\n",
    "    #####################\n",
    "    # Training parameters\n",
    "    #####################\n",
    "\n",
    "    # Network optimizer parameters (learning rate and momentum)\n",
    "    learning_rate = 1e-3\n",
    "    momentum = 0.9\n",
    "\n",
    "    # Learning rate decays. Dictionary of all decay values with their epoch {epoch: decay}.\n",
    "    lr_decays = {200: 0.2, 300: 0.2}\n",
    "\n",
    "    # Gradient clipping value (negative means no clipping)\n",
    "    grad_clip_norm = 100.0\n",
    "\n",
    "    # Augmentation parameters\n",
    "    augment_scale_anisotropic = True\n",
    "    augment_scale_min = 0.9\n",
    "    augment_scale_max = 1.1\n",
    "    augment_symmetries = [False, False, False]\n",
    "    augment_rotation = 'vertical'\n",
    "    augment_noise = 0.005\n",
    "    augment_color = 0.7\n",
    "\n",
    "    # Augment with occlusions (not implemented yet)\n",
    "    augment_occlusion = 'none'\n",
    "    augment_occlusion_ratio = 0.2\n",
    "    augment_occlusion_num = 1\n",
    "\n",
    "    # Regularization loss importance\n",
    "    weight_decay = 1e-3\n",
    "\n",
    "    # The way we balance segmentation loss DEPRECATED\n",
    "    segloss_balance = 'none'\n",
    "\n",
    "    # Choose weights for class (used in segmentation loss). Empty list for no weights\n",
    "    class_w = []\n",
    "\n",
    "    # Deformable offset loss\n",
    "    # 'point2point' fitting geometry by penalizing distance from deform point to input points\n",
    "    # 'point2plane' fitting geometry by penalizing distance from deform point to input point triplet (not implemented)\n",
    "    deform_fitting_mode = 'point2point'\n",
    "    deform_fitting_power = 1.0              # Multiplier for the fitting/repulsive loss\n",
    "    deform_lr_factor = 0.1                  # Multiplier for learning rate applied to the deformations\n",
    "    repulse_extent = 1.0                    # Distance of repulsion for deformed kernel points\n",
    "\n",
    "    # Number of batch\n",
    "    batch_num = 10\n",
    "    val_batch_num = 10\n",
    "\n",
    "    # Maximal number of epochs\n",
    "    max_epoch = 1000\n",
    "\n",
    "    # Number of steps per epochs\n",
    "    epoch_steps = 1000\n",
    "\n",
    "    # Number of validation examples per epoch\n",
    "    validation_size = 100\n",
    "\n",
    "    # Number of epoch between each checkpoint\n",
    "    checkpoint_gap = 50\n",
    "\n",
    "    # Do we nee to save convergence\n",
    "    saving = True\n",
    "    saving_path = None\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Class Initialyser\n",
    "        \"\"\"\n",
    "\n",
    "        # Number of layers\n",
    "        self.num_layers = len([block for block in self.architecture if 'pool' in block or 'strided' in block]) + 1\n",
    "\n",
    "        ###################\n",
    "        # Deform layer list\n",
    "        ###################\n",
    "        #\n",
    "        # List of boolean indicating which layer has a deformable convolution\n",
    "        #\n",
    "\n",
    "        layer_blocks = []\n",
    "        self.deform_layers = []\n",
    "        arch = self.architecture\n",
    "        for block_i, block in enumerate(arch):\n",
    "\n",
    "            # Get all blocks of the layer\n",
    "            if not ('pool' in block or 'strided' in block or 'global' in block or 'upsample' in block):\n",
    "                layer_blocks += [block]\n",
    "                continue\n",
    "\n",
    "            # Convolution neighbors indices\n",
    "            # *****************************\n",
    "\n",
    "            deform_layer = False\n",
    "            if layer_blocks:\n",
    "                if np.any(['deformable' in blck for blck in layer_blocks]):\n",
    "                    deform_layer = True\n",
    "\n",
    "            if 'pool' in block or 'strided' in block:\n",
    "                if 'deformable' in block:\n",
    "                    deform_layer = True\n",
    "\n",
    "            self.deform_layers += [deform_layer]\n",
    "            layer_blocks = []\n",
    "\n",
    "            # Stop when meeting a global pooling or upsampling\n",
    "            if 'global' in block or 'upsample' in block:\n",
    "                break\n",
    "\n",
    "    def load(self, path):\n",
    "\n",
    "        filename = join(path, 'parameters.txt')\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Class variable dictionary\n",
    "        for line in lines:\n",
    "            line_info = line.split()\n",
    "            if len(line_info) > 2 and line_info[0] != '#':\n",
    "\n",
    "                if line_info[2] == 'None':\n",
    "                    setattr(self, line_info[0], None)\n",
    "\n",
    "                elif line_info[0] == 'lr_decay_epochs':\n",
    "                    self.lr_decays = {int(b.split(':')[0]): float(b.split(':')[1]) for b in line_info[2:]}\n",
    "\n",
    "                elif line_info[0] == 'architecture':\n",
    "                    self.architecture = [b for b in line_info[2:]]\n",
    "\n",
    "                elif line_info[0] == 'augment_symmetries':\n",
    "                    self.augment_symmetries = [bool(int(b)) for b in line_info[2:]]\n",
    "\n",
    "                elif line_info[0] == 'num_classes':\n",
    "                    if len(line_info) > 3:\n",
    "                        self.num_classes = [int(c) for c in line_info[2:]]\n",
    "                    else:\n",
    "                        self.num_classes = int(line_info[2])\n",
    "\n",
    "                elif line_info[0] == 'class_w':\n",
    "                    self.class_w = [float(w) for w in line_info[2:]]\n",
    "\n",
    "                elif hasattr(self, line_info[0]):\n",
    "                    attr_type = type(getattr(self, line_info[0]))\n",
    "                    if attr_type == bool:\n",
    "                        setattr(self, line_info[0], attr_type(int(line_info[2])))\n",
    "                    else:\n",
    "                        setattr(self, line_info[0], attr_type(line_info[2]))\n",
    "\n",
    "        self.saving = True\n",
    "        self.saving_path = path\n",
    "        self.__init__()\n",
    "\n",
    "    def save(self):\n",
    "\n",
    "        with open(join(self.saving_path, 'parameters.txt'), \"w\") as text_file:\n",
    "\n",
    "            text_file.write('# -----------------------------------#\\n')\n",
    "            text_file.write('# Parameters of the training session #\\n')\n",
    "            text_file.write('# -----------------------------------#\\n\\n')\n",
    "\n",
    "            # Input parameters\n",
    "            text_file.write('# Input parameters\\n')\n",
    "            text_file.write('# ****************\\n\\n')\n",
    "            text_file.write('dataset = {:s}\\n'.format(self.dataset))\n",
    "            text_file.write('dataset_task = {:s}\\n'.format(self.dataset_task))\n",
    "            if type(self.num_classes) is list:\n",
    "                text_file.write('num_classes =')\n",
    "                for n in self.num_classes:\n",
    "                    text_file.write(' {:d}'.format(n))\n",
    "                text_file.write('\\n')\n",
    "            else:\n",
    "                text_file.write('num_classes = {:d}\\n'.format(self.num_classes))\n",
    "            text_file.write('in_points_dim = {:d}\\n'.format(self.in_points_dim))\n",
    "            text_file.write('in_features_dim = {:d}\\n'.format(self.in_features_dim))\n",
    "            text_file.write('in_radius = {:.6f}\\n'.format(self.in_radius))\n",
    "            text_file.write('input_threads = {:d}\\n\\n'.format(self.input_threads))\n",
    "\n",
    "            # Model parameters\n",
    "            text_file.write('# Model parameters\\n')\n",
    "            text_file.write('# ****************\\n\\n')\n",
    "\n",
    "            text_file.write('architecture =')\n",
    "            for a in self.architecture:\n",
    "                text_file.write(' {:s}'.format(a))\n",
    "            text_file.write('\\n')\n",
    "            text_file.write('equivar_mode = {:s}\\n'.format(self.equivar_mode))\n",
    "            text_file.write('invar_mode = {:s}\\n'.format(self.invar_mode))\n",
    "            text_file.write('num_layers = {:d}\\n'.format(self.num_layers))\n",
    "            text_file.write('first_features_dim = {:d}\\n'.format(self.first_features_dim))\n",
    "            text_file.write('use_batch_norm = {:d}\\n'.format(int(self.use_batch_norm)))\n",
    "            text_file.write('batch_norm_momentum = {:.6f}\\n\\n'.format(self.batch_norm_momentum))\n",
    "            text_file.write('segmentation_ratio = {:.6f}\\n\\n'.format(self.segmentation_ratio))\n",
    "\n",
    "            # KPConv parameters\n",
    "            text_file.write('# KPConv parameters\\n')\n",
    "            text_file.write('# *****************\\n\\n')\n",
    "\n",
    "            text_file.write('first_subsampling_dl = {:.6f}\\n'.format(self.first_subsampling_dl))\n",
    "            text_file.write('num_kernel_points = {:d}\\n'.format(self.num_kernel_points))\n",
    "            text_file.write('conv_radius = {:.6f}\\n'.format(self.conv_radius))\n",
    "            text_file.write('deform_radius = {:.6f}\\n'.format(self.deform_radius))\n",
    "            text_file.write('fixed_kernel_points = {:s}\\n'.format(self.fixed_kernel_points))\n",
    "            text_file.write('KP_extent = {:.6f}\\n'.format(self.KP_extent))\n",
    "            text_file.write('KP_influence = {:s}\\n'.format(self.KP_influence))\n",
    "            text_file.write('aggregation_mode = {:s}\\n'.format(self.aggregation_mode))\n",
    "            text_file.write('modulated = {:d}\\n'.format(int(self.modulated)))\n",
    "            text_file.write('n_frames = {:d}\\n'.format(self.n_frames))\n",
    "            text_file.write('max_in_points = {:d}\\n\\n'.format(self.max_in_points))\n",
    "            text_file.write('max_val_points = {:d}\\n\\n'.format(self.max_val_points))\n",
    "            text_file.write('val_radius = {:.6f}\\n\\n'.format(self.val_radius))\n",
    "\n",
    "            # Training parameters\n",
    "            text_file.write('# Training parameters\\n')\n",
    "            text_file.write('# *******************\\n\\n')\n",
    "\n",
    "            text_file.write('learning_rate = {:f}\\n'.format(self.learning_rate))\n",
    "            text_file.write('momentum = {:f}\\n'.format(self.momentum))\n",
    "            text_file.write('lr_decay_epochs =')\n",
    "            for e, d in self.lr_decays.items():\n",
    "                text_file.write(' {:d}:{:f}'.format(e, d))\n",
    "            text_file.write('\\n')\n",
    "            text_file.write('grad_clip_norm = {:f}\\n\\n'.format(self.grad_clip_norm))\n",
    "\n",
    "\n",
    "            text_file.write('augment_symmetries =')\n",
    "            for a in self.augment_symmetries:\n",
    "                text_file.write(' {:d}'.format(int(a)))\n",
    "            text_file.write('\\n')\n",
    "            text_file.write('augment_rotation = {:s}\\n'.format(self.augment_rotation))\n",
    "            text_file.write('augment_noise = {:f}\\n'.format(self.augment_noise))\n",
    "            text_file.write('augment_occlusion = {:s}\\n'.format(self.augment_occlusion))\n",
    "            text_file.write('augment_occlusion_ratio = {:.6f}\\n'.format(self.augment_occlusion_ratio))\n",
    "            text_file.write('augment_occlusion_num = {:d}\\n'.format(self.augment_occlusion_num))\n",
    "            text_file.write('augment_scale_anisotropic = {:d}\\n'.format(int(self.augment_scale_anisotropic)))\n",
    "            text_file.write('augment_scale_min = {:.6f}\\n'.format(self.augment_scale_min))\n",
    "            text_file.write('augment_scale_max = {:.6f}\\n'.format(self.augment_scale_max))\n",
    "            text_file.write('augment_color = {:.6f}\\n\\n'.format(self.augment_color))\n",
    "\n",
    "            text_file.write('weight_decay = {:f}\\n'.format(self.weight_decay))\n",
    "            text_file.write('segloss_balance = {:s}\\n'.format(self.segloss_balance))\n",
    "            text_file.write('class_w =')\n",
    "            for a in self.class_w:\n",
    "                text_file.write(' {:.6f}'.format(a))\n",
    "            text_file.write('\\n')\n",
    "            text_file.write('deform_fitting_mode = {:s}\\n'.format(self.deform_fitting_mode))\n",
    "            text_file.write('deform_fitting_power = {:.6f}\\n'.format(self.deform_fitting_power))\n",
    "            text_file.write('deform_lr_factor = {:.6f}\\n'.format(self.deform_lr_factor))\n",
    "            text_file.write('repulse_extent = {:.6f}\\n'.format(self.repulse_extent))\n",
    "            text_file.write('batch_num = {:d}\\n'.format(self.batch_num))\n",
    "            text_file.write('val_batch_num = {:d}\\n'.format(self.val_batch_num))\n",
    "            text_file.write('max_epoch = {:d}\\n'.format(self.max_epoch))\n",
    "            if self.epoch_steps is None:\n",
    "                text_file.write('epoch_steps = None\\n')\n",
    "            else:\n",
    "                text_file.write('epoch_steps = {:d}\\n'.format(self.epoch_steps))\n",
    "            text_file.write('validation_size = {:d}\\n'.format(self.validation_size))\n",
    "            text_file.write('checkpoint_gap = {:d}\\n'.format(self.checkpoint_gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9/850547889.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(target_dir, checkpoint_file), map_location=torch.device(device))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "KPFCNN.__init__() missing 3 required positional arguments: 'config', 'lbl_values', and 'ign_lbls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(target_dir, checkpoint_file), map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device))\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mKPFCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: KPFCNN.__init__() missing 3 required positional arguments: 'config', 'lbl_values', and 'ign_lbls'"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint = torch.load(os.path.join(target_dir, checkpoint_file), map_location=torch.device(device))\n",
    "model = KPFCNN(Config(), ).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
